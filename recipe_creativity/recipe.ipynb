{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "unaffected by use or exposure\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['creative', 'originative']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns = wordnet.synsets(\"new\")\n",
    "print(syns)\n",
    "print(wordnet.synset('new.a.06').definition())\n",
    "# we should use creative.a.02, by definition.\n",
    "# but it doesn't have synonyms.\n",
    "# so we use also creative.a.01, and gives\n",
    "\n",
    "wordnet.synset('creative.a.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "first_word = wordnet.synset(\"creative.a.01\")\n",
    "second_word = wordnet.synset(\"originative.a.01\")\n",
    "print('Similarity: ' + str(first_word.wup_similarity(second_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New', 'refreshing', 'interest', 'inventive', 'raw', 'genius', 'Einstein', 'maven', 'clever', 'innovational', 'ace', 'star', 'superstar', 'groundbreaking', 'brainiac', 'sensation', 'concern', 'unexampled', 'occupy', 'fresh', 'newly', 'adept', 'worry', 'champion', 'modern', 'wizard', 'imaginative', 'wizardry', 'Modern', 'mastermind', 'ingenious', 'advanced', 'matter_to', 'mavin', 'wiz', 'new', 'cunning', 'whiz', 'originative', 'whizz', 'flair', 'brilliance', 'young', 'interesting', 'virtuoso', 'creative', 'hotshot', 'freshly', 'forward-looking', 'novel', 'brain', 'newfangled', 'innovative']\n",
      "['old', 'worn', 'uninteresting', 'bore', 'uncreative']\n",
      "['hold', 'turn_out', 'abide', 'deport', 'drawn', 'fag_out', 'acquit', 'expect', 'gestate', 'have_a_bun_in_the_oven', 'eager', 'bore-hole', 'bear', 'gauge', 'sometime', 'have', 'haggard', 'accept', 'bust', 'erstwhile', 'honest-to-goodness', 'deliver', 'take_over', 'comport', 'stand', 'outwear', 'drill_hole', 'older', 'honest-to-god', 'raddled', 'wear_thin', 'stick_out', 'stomach', 'careworn', 'uninteresting', 'aegir', 'wear_off', 'fatigue', 'fall_apart', 'sure-enough', 'bore', 'digest', 'wear_out', 'Old', 'give_birth', 'onetime', 'caliber', 'birth', 'weary', 'tire', 'calibre', 'former', 'behave', 'old', 'drill', 'assume', 'fag', 'previous', 'put_on', 'contain', 'wear', 'tolerate', 'yield', 'carry', 'support', 'break', 'jade', 'suffer', 'dullard', 'tire_out', 'one-time', 'tidal_bore', 'put_up', 'wear_upon', 'pay', 'hold_out', 'get_into', 'brook', 'uncreative', 'conduct', 'quondam', 'endure', 'wear_down', 'worn', 'eagre', 'don', 'have_on']\n"
     ]
    }
   ],
   "source": [
    "creativity_dict = ['creative', 'new', 'novel', 'interesting', 'genius', \n",
    "                   'imaginative', 'ingenious','innovative', 'inventive']\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for creativity_word in creativity_dict:\n",
    "    for syn in wordnet.synsets(creativity_word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "            if lemma.antonyms():\n",
    "                for a in lemma.antonyms():\n",
    "                    antonyms.append(a.name())\n",
    "\n",
    "creative_list = list(set(synonyms))\n",
    "antonyms = list(set(antonyms))\n",
    "print(creative_list)\n",
    "print(antonyms)\n",
    "\n",
    "noncreative_list = []\n",
    "for word in antonyms:\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            noncreative_list.append(lemma.name())\n",
    "\n",
    "print(list(set(noncreative_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Google News Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-80fded4efba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gensim-data/word2vec-google-news-300/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m                     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb' '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mREAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_compression.py\u001b[0m in \u001b[0;36m_check_not_closed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I/O operation on closed file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mclosed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "pretrained_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "google_model = models.KeyedVectors.load_word2vec_format(\n",
    "    '/Users/nessyliu/Desktop/RA/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(pretrained_model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pretrained_model['creative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Similarity\n",
    "# pairs = [\n",
    "#     ('creative', 'interesting'),   \n",
    "#     ('banana', 'apple') \n",
    "# ]\n",
    "# for w1, w2 in pairs:\n",
    "#     print('%r\\t%r\\t%.2f' % (w1, w2, pretrained_model.similarity(w1, w2)))\n",
    "    \n",
    "# top similar words to the words in our dict\n",
    "top_similar_1 = list(set(pretrained_model.most_similar(positive=creativity_dict, topn=20)))\n",
    "print(\"similar words to the words in our dict:\")\n",
    "print(top_similar_1)\n",
    "\n",
    "# top similar words to \"uncreative\"\n",
    "print(\"\\nsimilar words to 'uncreative':\")\n",
    "print(list(set(pretrained_model.most_similar(positive=['creative'], topn=10))))\n",
    "\n",
    "\n",
    "# top negative words to the words in our dict\n",
    "top_negative_1 = list(set(pretrained_model.most_similar(negative=creativity_dict, topn=20)))\n",
    "print(\"\\nnegative words to the words in our dict:\")\n",
    "print(top_negative_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build our own word2vec model with review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import utils\n",
    "import tempfile\n",
    "\n",
    "class ReviewCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (list of tokens).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        df = pd.read_csv('/Users/nessyliu/Desktop/RA/AllReviews_26thNov2019.csv')\n",
    "        for review in df['review_text']:\n",
    "            yield utils.simple_preprocess(review)\n",
    "\n",
    "reviews = ReviewCorpus()\n",
    "review_model = gensim.models.Word2Vec(reviews, min_count=5, size=300, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "with tempfile.NamedTemporaryFile(prefix='review-model-', delete=False) as tmp:\n",
    "        review_model_path = tmp.name\n",
    "        print(review_model_path)\n",
    "        review_model.save(review_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_review_model = gensim.models.Word2Vec.load(review_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(only_review_model.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Similarity\n",
    "# pairs = [\n",
    "#     ('king', 'queen'),   \n",
    "#     ('banana', 'apple') \n",
    "# ]\n",
    "# for w1, w2 in pairs:\n",
    "#     print('%r\\t%r\\t%.2f' % (w1, w2, only_review_model.wv.similarity(w1, w2)))\n",
    "\n",
    "\n",
    "    \n",
    "top_similar_2 = list(set(only_review_model.wv.most_similar(positive=creativity_dict, topn=20)))\n",
    "print(\"similar words to the words in our dict:\")\n",
    "print(top_similar_2)\n",
    "\n",
    "# # top similar words to \"uncreative\"\n",
    "# print(\"\\nsimilar words to 'uncreative':\")\n",
    "# print(list(set(only_review_model.most_similar(positive=['uncreative'], topn=10))))\n",
    "\n",
    "# top negative words to the words in our dict\n",
    "top_negative_2 = list(set(only_review_model.wv.most_similar(negative=creativity_dict, topn=20)))\n",
    "print(\"\\nnegative words to the words in our dict:\")\n",
    "print(top_negative_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(only_review_model.wv['creative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with our review data based on pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(pretrained_model.vocab)))\n",
    "\n",
    "df = pd.read_csv('/Users/nessyliu/Desktop/RA/AllReviews_26thNov2019.csv')\n",
    "review_list = []\n",
    "for review in df['review_text']:\n",
    "    review_list.append(utils.simple_preprocess(review))\n",
    "print(len(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = gensim.models.Word2Vec(min_count=5, size=300, window = 5)\n",
    "\n",
    "new_model.build_vocab(review_list)\n",
    "new_model.intersect_word2vec_format(\"/Users/nessyliu/Desktop/RA/GoogleNews-vectors-negative300.bin\", \n",
    "                                  binary=True, lockf=1.0)\n",
    "new_model.train(review_list, total_examples=len(review_list), epochs=new_model.epochs)\n",
    "\n",
    "print(len(list(new_model.wv.vocab)))\n",
    "\n",
    "# pairs = [\n",
    "#     ('creative', 'interesting'),   \n",
    "#     ('banana', 'apple') \n",
    "# ]\n",
    "# for w1, w2 in pairs:\n",
    "#     print('%r\\t%r\\t%.2f' % (w1, w2, new_model.wv.similarity(w1, w2)))\n",
    "\n",
    "print(new_model.wv['creative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top similar words to the words in our dict\n",
    "top_similar_3 = list(set(new_model.wv.most_similar(positive=creativity_dict, topn=100)))\n",
    "print(\"similar words to the words in our dict:\")\n",
    "print(top_similar_3)\n",
    "\n",
    "# # top similar words to \"uncreative\"\n",
    "# print(\"\\nsimilar words to 'uncreative':\")\n",
    "# print(list(set(new_model.most_similar(positive=['uncreative'], topn=10))))\n",
    "\n",
    "# top negative words to the words in our dict\n",
    "top_negative_3 = list(set(new_model.wv.most_similar(negative=creativity_dict, topn=20)))\n",
    "print(\"\\nnegative words to the words in our dict:\")\n",
    "print(top_negative_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncreative_3 = list(set(new_model.wv.most_similar(positive=['old', 'bore', 'worn', 'uninteresting',\n",
    "                                                            'uninteresting','uninspired',\n",
    "                                                            'boring','bland'], topn=30)))\n",
    "print(\"similar words to the 'uncreative' words:\")\n",
    "print(uncreative_3)\n",
    "\n",
    "for i in uncreative_3:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile(prefix='new-model-', delete=False) as tmp:\n",
    "        new_model_path = tmp.name\n",
    "        print(new_model_path)\n",
    "        new_model.save(new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_load = gensim.models.Word2Vec.load(new_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-82b2d8403b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/nessyliu/Documents/GitHub/Recipe_Project/OUT/word2vec_model/model_3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "gensim.models.Word2Vec.load('/Users/nessyliu/Documents/GitHub/Recipe_Project/OUT/word2vec_model/model_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
