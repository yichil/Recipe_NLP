{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import csv\n",
    "import itertools\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from nltk import word_tokenize, pos_tag, corpus\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict, Counter\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets\n",
    "df_directions = pd.read_excel('/Users/nessyliu/Desktop/RA/AllRecipes_Directions_2019-11-26.xlsx')\n",
    "df = pd.read_csv('/Users/nessyliu/Desktop/RA/AllReviews_26thNov2019.csv')\n",
    "df_ingredients_raw = pd.read_csv('/Users/nessyliu/Desktop/RA/part_2/Ingredients.csv')\n",
    "df_cluster = pd.read_excel('/Users/nessyliu/Desktop/RA/part_2/Cluster_names.xlsx')\n",
    "\n",
    "df_ingre_clean = pd.read_csv('/Users/nessyliu/Desktop/RA/part_2/ingredients_after_text_cleaning.csv')\n",
    "df_mod = pd.read_excel('/Users/nessyliu/Desktop/RA/part_2/Final_clusters_mod.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_directions_list = []\n",
    "recipe_id_list = list(set(df_directions.recipe_id.tolist()))\n",
    "for recipe_id in recipe_id_list:\n",
    "    full_dir_this_recipe = ' '.join(df_directions.loc[df_directions['recipe_id']==recipe_id, 'directions_step_text'])\n",
    "    full_directions_list.append(full_dir_this_recipe)\n",
    "dict_recipe_direction = dict(zip(recipe_id_list, full_directions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of cluster names with spaces (e.g. apple juice) sorted by number of words in each name, \n",
    "# e.g. \"apple juice\" should appear before \"juice\"\n",
    "cluster_name_orig_list = df_cluster.cluster_name.tolist()\n",
    "cluster_name_orig_list.sort(key=lambda x: len(x.split()), reverse=True)\n",
    "\n",
    "# create dict to map recipe_id to ingredient_ids\n",
    "df_ingredients = df_ingredients_raw.groupby('recipe_id')['ingredient_id'].apply(list).reset_index(name ='ingredients')\n",
    "dict_recipe_ingredients = dict(zip(df_ingredients.recipe_id, df_ingredients.ingredients))\n",
    "\n",
    "# create dict to map ingredient_id to cluster_name\n",
    "df_cluster = df_cluster.replace(' ', '_', regex=True)\n",
    "dict_ingredient_clustername = dict(zip(df_cluster.ingredient_id, df_cluster.cluster_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization dict\n",
    "lemma_list = pd.read_csv('/Users/nessyliu/Desktop/RA/lemma_list.csv')\n",
    "lemma_dict = lemma_list.set_index('word_list').to_dict()['lemma_list']\n",
    "\n",
    "def lemmatization(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace('sugar substitute', 'sweetener').replace('french fry','frenchfry')\n",
    "    \n",
    "    text = text.replace('lowfat','low fat').replace('nonfat','non fat').replace('glutenfree','gluten free').replace('corn flakes','cornflakes')\n",
    "    \n",
    "    text = text.replace('coconutmilk',\n",
    "                        'coconut milk').replace('almondmilk',\n",
    "                                                'almond milk').replace('crab meat',\n",
    "                                                                       'crabmeat').replace('starfruit', \n",
    "                                                                                           'star fruit').replace('breadcrumb', 'bread crumb')\n",
    "    \n",
    "    # use the custom lemma dict first\n",
    "    text = \" \".join(str(lemma_dict.get(word, word)) for word in text.split())\n",
    "    \n",
    "    # then use the WordNetLemmatizer from nltk\n",
    "    wnl = WordNetLemmatizer()\n",
    "    # lemmatize each word based on its pos tagging\n",
    "    text_after = \" \".join([wnl.lemmatize(i,j[0]) if j[0] in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(text))])    \n",
    "    return text_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_exclude_list = ['purpose','extra', 'whole', 'frying', 'cut', \n",
    "                        'sun', 'baby', 'five', 'star', 'non', 'dash', 'style',\n",
    "                        'white', 'green', 'black', 'red', 'pink', 'yellow', 'brown', 'golden', 'blue', \n",
    "                        'color', 'colored', 'half', 'alternative',\n",
    "                        'ground', 'sea', 'part', 'baked', 'raw', 'new', 'active',\n",
    "                        'italian', 'dark', 'light', 'fresh', 'sweet', 'candied',\n",
    "                        'dried', 'dry', 'heavy', 'condensed', 'firm', 'soft', 'free', \n",
    "                        'mixed', 'flavored', 'evaporated', 'peeled', 'pickled','cooked','chopped', 'broken',\n",
    "                        'hot', 'self', 'rising', 'split', 'cooking', 'stewed',\n",
    "                        'de', 'dr']\n",
    "unigram_exclude_list += corpus.stopwords.words('english')\n",
    "\n",
    "ngram_exclude_list = ['all_purpose', 'purpose_flour', 'free_all']\n",
    "\n",
    "ngram_not_start_end = ['for', 'of', 'and', 'with', 'in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict for appearance times for each cluster name\n",
    "dict_cluster_count = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea7bc3fbc4e41119f443564a4a64c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=69121.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_recipe_id_list = []\n",
    "perfect_match_list = []\n",
    "partial_match_list = []\n",
    "no_match_list = []\n",
    "recipe_list = []\n",
    "direction_list = []\n",
    "lemmatized_direction_list = []\n",
    "\n",
    "num_ingredient_list = []\n",
    "num_perfect_match_list = []\n",
    "num_partial_match_list = []\n",
    "num_no_match_list = []\n",
    "\n",
    "count_invalid = 0\n",
    "for recipe_id in tqdm(recipe_id_list):\n",
    "    \n",
    "    # list of ingre that appeared exactly the same in the direction\n",
    "    perfect_match_this_recipe = []\n",
    "    # list of ingre that matched shorter form in the direction\n",
    "    partial_match_this_recipe = []\n",
    "    # list fo ingre that are not matched\n",
    "    no_match_this_recipe = []\n",
    "    \n",
    "    # count number of ingredients\n",
    "    num_perfect_match_this_recipe = 0\n",
    "    num_partial_match_this_recipe = 0\n",
    "    num_no_match_this_recipe = 0\n",
    "\n",
    "    # get the ingredients of this recipe id\n",
    "    try:\n",
    "        recipe = [dict_ingredient_clustername[ingre_id] for ingre_id in dict_recipe_ingredients[recipe_id]]\n",
    "        valid_recipe_id_list.append(recipe_id)\n",
    "        \n",
    "        recipe.sort(key=lambda x: len(x.split('_')), reverse=True)\n",
    "        \n",
    "        # get the direction of this recipe id\n",
    "        direction = dict_recipe_direction[recipe_id]\n",
    "        lemmatized_direction = lemmatization(dict_recipe_direction[recipe_id])\n",
    "        # map each ingredient to the words in the direction\n",
    "        for ingre in recipe:\n",
    "            dict_cluster_count[ingre] += 1\n",
    "            ingre_tokens = ingre.split('_')\n",
    "            ingre_original = ' '.join(ingre_tokens)\n",
    "            if ingre_original in direction or ingre_original in lemmatized_direction:\n",
    "                perfect_match_this_recipe.append(ingre)\n",
    "                num_perfect_match_this_recipe += 1\n",
    "            else:\n",
    "                partial_match_this_ingre = []\n",
    "                if len(ingre_tokens) > 1:\n",
    "                    n = len(ingre_tokens)-1\n",
    "                    while n > 0: # length of the short form\n",
    "                        for i in range(0,len(ingre_tokens)-n+1): # start index in the full cluster name\n",
    "                            short_form = ' '.join(ingre_tokens[i:i+n])\n",
    "                            if short_form in direction or short_form in lemmatized_direction:\n",
    "                                # the short form is a map to the ingre\n",
    "                                # don't break the loop if matched, cuz 'oil for frying' may end up matched 'for frying' and break\n",
    "                                # but ignore subsequence part, e.g. if 'firm tofu' is matched, ignore 'firm' & 'tofu'\n",
    "                                if not any(short_form in existing_short_form for existing_short_form in partial_match_this_ingre):\n",
    "                                    if n > 1: # if the short form >=2 words\n",
    "                                        if short_form.replace(' ','_') not in ngram_exclude_list\\\n",
    "                                            and ingre_tokens[i] not in ngram_not_start_end \\\n",
    "                                            and ingre_tokens[i+n-1] not in ngram_not_start_end:\n",
    "                                                # if the short form not in ngram_exclude_list\n",
    "                                                # and the short form does not start or end with words in ngram_not_start_end list\n",
    "                                                partial_match_this_ingre.append(short_form.replace(' ','_'))\n",
    "                                    else: # if the short form is unigram\n",
    "                                        if short_form not in unigram_exclude_list:\n",
    "                                            partial_match_this_ingre.append(short_form)\n",
    "\n",
    "                        n -= 1\n",
    "                if len(partial_match_this_ingre) == 0:\n",
    "                    no_match_this_recipe.append(ingre)\n",
    "                    num_no_match_this_recipe += 1\n",
    "                else:\n",
    "                    partial_match_this_recipe.append([ingre, partial_match_this_ingre])\n",
    "                    num_partial_match_this_recipe += 1\n",
    "\n",
    "        recipe_list.append(recipe)\n",
    "        direction_list.append(direction)\n",
    "        lemmatized_direction_list.append(lemmatized_direction)\n",
    "        perfect_match_list.append(perfect_match_this_recipe)\n",
    "        partial_match_list.append(partial_match_this_recipe)\n",
    "        no_match_list.append(no_match_this_recipe)\n",
    "        num_perfect_match_list.append(num_perfect_match_this_recipe)\n",
    "        num_partial_match_list.append(num_partial_match_this_recipe)\n",
    "        num_no_match_list.append(num_no_match_this_recipe)\n",
    "        num_ingredient_list.append(len(recipe))\n",
    "    \n",
    "    \n",
    "    except:\n",
    "        count_invalid += 1\n",
    "        continue\n",
    "\n",
    "matched_df = pd.DataFrame({\n",
    "    'recipe_id': valid_recipe_id_list, \n",
    "    'recipe': recipe_list,\n",
    "    'direction': direction_list,\n",
    "    'lemma_direction': lemmatized_direction_list,\n",
    "    'perfect_match': perfect_match_list,\n",
    "    'partial_match': partial_match_list,\n",
    "    'no_match': no_match_list,\n",
    "    'num_ingredient': num_ingredient_list,\n",
    "    'num_perfect_match': num_perfect_match_list,\n",
    "    'num_partial_match': num_partial_match_list,\n",
    "    'num_no_match': num_no_match_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(matched_df.shape)\n",
    "matched_df.to_csv('/Users/nessyliu/Desktop/RA/part_2/result/matched_directions_orgin_w5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the 1-gram to (n-1)-gram shorter forms\n",
    "# ingre_tokens = ['fresh','white','chocolate','chips']\n",
    "\n",
    "# n = len(ingre_tokens)-1\n",
    "# while n > 0:\n",
    "#     for i in range(0,len(ingre_tokens)-n+1):\n",
    "#         short_form = ' '.join(ingre_tokens[i:i+n])\n",
    "#         print(short_form)\n",
    "#     n = n-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count of ingredients not matched in all recipe directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_match_list_flat = list(itertools.chain.from_iterable(no_match_list))\n",
    "count_no_match = Counter(no_match_list_flat)\n",
    "count_no_match = count_no_match.most_common()\n",
    "len(count_no_match) # ingre at least not matched in 1 recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Label2 in Final_clusters_mod for synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "half_and_half 734\n",
      "['fat']\n",
      "salt 662\n",
      "['sodium']\n",
      "garbanzo_bean 223\n",
      "['chickpea', 'liquid', 'gravy', 'crumb', 'bengal', 'gram']\n",
      "green_onion 202\n",
      "['scallion']\n",
      "ground_cinnamon 195\n",
      "['mccormick', 'ceylon']\n",
      "margarine 187\n",
      "['vegan', 'spread', 'fat', 'butter', 'soy', 'oil', 'corn', 'dairy', 'vegetable']\n",
      "water 186\n",
      "['orange', 'flower', 'spring']\n",
      "butter 182\n",
      "['sunbutter', 'variety', 'garlic', 'land']\n",
      "pecan 181\n",
      "['decoration', 'extract']\n",
      "graham_cracker 160\n",
      "['crust', 'chocolate', 'pie', 'crumb', 'mini', 'cinnamon', 'fat', 'deep', 'dish', 'cooky', 'bear', 'snack', 'honey', 'maid']\n",
      "pie_crust 143\n",
      "['shell', 'tart', 'pastry', 'deep', 'dish', 'shortbread', 'mix', 'wheat', 'chocolate', 'sandwich', 'cookie', 'pillsbury', 'box', 'mini', 'phyllo', 'shortcake', 'recipe', 'ball', 'dough']\n",
      "ground_nutmeg 124\n",
      "['mccormick']\n",
      "egg 123\n",
      "[\"eggland's\", 'strip']\n",
      "vanilla_extract 120\n",
      "['imitation', 'mccormick', 'pure']\n",
      "crabmeat 109\n",
      "['crab', 'imitation', 'king', 'leg', 'thawed', 'alaskan', 'shell', 'snow', 'cluster']\n",
      "ground_beef 93\n",
      "['lean', 'meatloaf', 'patty', 'hamburger', 'leftover', 'mix', 'round', 'chuck', 'burger']\n",
      "ground_black_pepper 87\n",
      "['mccormick', 'coarse']\n",
      "vegetable_oil 82\n",
      "['drop', 'hazelnut', 'flaxseed', 'garlic', 'cinnamon', 'medium', 'chain', 'triglyceride', 'almond', 'palm', 'orange', 'macadamia', 'nut', 'anise', 'pina', 'colada', 'candy', 'tomato', 'lemon', 'pumpkin', 'seed', 'popcorn', 'rice', 'bran', 'nutmeg', 'basil', 'poppyseed', 'ginger', 'mustard', 'substitute', 'pistachio', 'coriander']\n",
      "ground_clove 81\n",
      "['mccormick']\n",
      "green_chile_pepper 78\n",
      "['hatch', 'mexico', 'chili']\n",
      "baking_powder 73\n",
      "['sodium']\n",
      "honey 72\n",
      "['aunt', 'powder', 'butter']\n",
      "spaghetti 69\n",
      "['spaghettini', 'spinach', 'pasta', 'thin', 'wheat']\n",
      "macaroni 69\n",
      "['elbow', 'wheat', 'multi', 'grain', 'multigrain']\n",
      "luncheon_meat 64\n",
      "['deli', 'turkey', 'thin', 'slice', 'beef', 'lite', 'sodium', 'chicken', 'pastrami', 'capicola', 'mortadella', 'meatless', 'hormel', 'choice']\n",
      "milk 64\n",
      "['lactose']\n",
      "onion 63\n",
      "['purple', 'medium', 'ring', 'cipollini', 'welsh', 'matchstick', 'sauteed', 'butter']\n",
      "bread_crumb 62\n",
      "['panko', 'wheat', 'garlic', 'herb', 'parmesan', 'cheese', 'breadcrumb']\n",
      "raisin 60\n",
      "['chocolate', 'yogurt', 'sultana', 'paste', 'cinnamon']\n",
      "adobo_sauce 59\n",
      "['chipotle', 'pepper']\n",
      "garlic 59\n",
      "['clove', 'oil', 'juice', 'elephant']\n",
      "sirloin 59\n",
      "['lean', 'cold', 'top', 'beef']\n",
      "milk_powder 57\n",
      "['strawberry', 'skim', 'fat', 'instant']\n",
      "jam 54\n",
      "['preserve', 'cherry', 'mango', 'plum', 'lingonberry', 'flavor', 'fruit', 'lemon', 'marmalade', 'cranberry']\n",
      "carrot 53\n",
      "['food', 'juice', 'pureed']\n",
      "italian_seasoning 51\n",
      "['mccormick', 'perfect', 'pinch', 'blend']\n",
      "potato 51\n",
      "['instant', 'fry', 'flake', 'leftover', 'mix', 'au', 'gratin', 'garlic', 'salad', 'idahoan', 'cheese', 'purple', 'creamer', 'crinkle']\n",
      "buttermilk 51\n",
      "['fat', 'powder']\n",
      "bread 50\n",
      "['loaf', 'multigrain', 'toast', \"nature's\", 'cubed', 'cinnamon', 'country', 'potato', 'sheet', 'lavash', 'sandwich', 'soda', 'farl', 'pumpkin', 'friendship', 'starter', 'garlic', 'oatnut', 'stale', 'cranberry', 'walnut', 'crosswise', 'swirl', 'banana', 'torn', 'cube', 'calorie', 'oatmeal', 'schr', 'artisan', 'baker', 'grain', 'seed', 'zwieback', 'anisette', 'bruschetta']\n",
      "cornflakes_cereal 49\n",
      "['honey', 'corn', 'flake', 'nut', 'flavor', 'cornflake', 'crumb', 'sugar']\n",
      "parsley 47\n",
      "['leaf', 'flake', 'root', 'chunk', 'mccormick', 'paste', 'chervil']\n",
      "liqueur 47\n",
      "['chocolate', 'pumpkin', 'strawberry', 'honey', 'whiskey', 'daiquiri', 'mix', 'benedictine', 'creme', 'cassis', 'galliano', 'elderflower', 'praline', 'amaro', 'ginger', 'pomegranate', 'blackberry', 'maraschino', 'lychee', 'herb', 'spice', 'almond', 'maple', 'cranberry', 'coconut', 'peach', 'mai', 'tai', 'cocktail', 'chartreuse', 'cachaca', 'bottle', 'malt', 'liquor']\n",
      "bean 47\n",
      "['soybean', 'soup', 'paste', 'dip', 'adzuki', 'curd', 'thread', 'bacon', 'mayocoba', 'soy', 'salad', 'garlic', 'sauce', 'soldier', 'liquid', 'corona', 'tri', 'blend', 'piece', 'block']\n",
      "cod 47\n",
      "['fillet', 'fish', 'lingcod', 'skin', 'bone']\n",
      "almond 47\n",
      "['honey', 'marcona', 'chocolate', 'cocoa']\n",
      "soy_sauce 45\n",
      "['sodium', 'tamari', 'lite']\n",
      "hash_brown 44\n",
      "['potato', 'patty', \"o'brien\"]\n",
      "syrup 43\n",
      "['cane', 'sugar', 'simple', 'chocolate', 'pancake', 'glucose', 'eagle', 'pomegranate']\n",
      "round_steak 43\n",
      "['eye']\n",
      "strawberry 43\n",
      "['freeze', 'puree']\n",
      "beef_sirloin 42\n",
      "['boneless', 'steak', 'room', 'temperature']\n",
      "other_fish 42\n",
      "['fillet', 'mackerel', 'sardine', 'bluefish', 'perch', 'oil', 'bonito', 'orange', 'roughy', 'grouper', 'monkfish', 'bass', 'hake', 'amberjack', 'butterfish', 'pollock', 'rockfish', 'pompano', 'drum', 'stick', 'milkfish', 'water', 'skinless', 'boneless', 'head', 'bone', 'char', 'smelt', 'mullet', 'pacific', 'saury']\n",
      "beef_chuck 41\n",
      "['roast', 'cubed', 'angus']\n"
     ]
    }
   ],
   "source": [
    "no_label2 = []\n",
    "\n",
    "# To find synonyms:\n",
    "# For each missing ingredient, check the label2 in Final_clusters_mod.csv\n",
    "for (missing_ingre, missing_count) in count_no_match:\n",
    "    missing_ingre_unigrams = missing_ingre.split('_')\n",
    "    all_label2 = ' '.join(df_mod.loc[df_mod['cluster_name'] == ' '.join(missing_ingre_unigrams),'label2'])\n",
    "    unigrams = all_label2.split(' ')\n",
    "    count_unigrams = Counter(unigrams).most_common()\n",
    "    if len(count_unigrams)>0:\n",
    "        possible_names = [unigram for (unigram, count) in count_unigrams if unigram \\\n",
    "                          not in unigram_exclude_list+missing_ingre_unigrams+['']\\\n",
    "                          and pos_tag([unigram])[0][1]=='NN']\n",
    "    if len(possible_names)>0:\n",
    "        if missing_count > 40:\n",
    "            print(missing_ingre, missing_count)\n",
    "            print(possible_names)\n",
    "    else:\n",
    "        no_label2.append(missing_ingre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_label2 # does not have possible unigram name in label2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients missing pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing (no match) pattern\n",
    "all_no_match_unique = set(no_match_list_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bcabef02ec4fffa4e6cf8845b5b325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "matched_df_missing = matched_df.loc[:,['recipe_id', 'no_match']]\n",
    "\n",
    "no_match_span_df = pd.concat([pd.Series(row['recipe_id'], row['no_match']) for _, row in tqdm(matched_df_missing.iterrows())]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7824, 762)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_match_span_df.columns = ['missing_ingredient', 'recipe_id']\n",
    "missing_matrix = pd.pivot_table(no_match_span_df, values='missing_ingredient', index=['recipe_id'], columns=['missing_ingredient'], aggfunc=lambda x: len(x))\n",
    "\n",
    "missing_matrix = missing_matrix.fillna(0)\n",
    "\n",
    "missing_matrix.shape # row is each recipe, col is each (missing) ingre\n",
    "\n",
    "# missing_matrix.insert(loc=0, column='recipe_id', value=missing_matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57709 recipes in total.\n",
      "There are 7824 recipes with at least 1 ingredient not matched.\n",
      "There are 762 unique ingredients not matched in at least 1 recipe out of 57709 recipes.\n",
      "There are 675 unique ingredients not matched in <= 20 % of its appearances.\n",
      "There are 87 unique ingredients not matched in > 20 % of its appearances.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Not matched in % of its appearances')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAEJCAYAAABi2tVNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAas0lEQVR4nO3debRlZXnn8e9PEBkcEClZCOIFU3Fo2rE0iLZB0BWnKG2DQ6OWiiHGAXEAS9MR7dYVjDglaTElKmg7IRIBS1G7BGeJVaCCIGoAsRShVAQcWkSe/mPvG4/XO+w7nHPu3fX9rHXW2fvd03P3PVXPfd/9nvdNVSFJkla2W407AEmStHgmdEmSesCELklSD5jQJUnqARO6JEk9YEKXJKkHhpbQk7w7ybVJLh4o2y3JZ5J8t32/Y1ueJP+Y5HtJvpnkAcOKS5KkPhpmDf0U4NFTytYBG6tqNbCxXQd4DLC6fR0FnDTEuCRJ6p0Mc2CZJBPAx6tq/3b9MuCgqro6yZ7AeVV1jyT/0i5/cOp+s51/9913r4mJiaHFL0nScrJ58+afVNWq6bZtP+JY9phM0m1Sv3Nbvhfwg4H9trRlsyb0iYkJNm3aNJRAJUlabpJ8f6Zty6VTXKYpm7bpIMlRSTYl2bR169YhhyVJ0sow6oR+TdvUTvt+bVu+BbjrwH57Az+a7gRVtb6q1lTVmlWrpm11kCRpmzPqhH4WsLZdXgucOVD+zLa3+wHA9XM9P5ckSb83tGfoST4IHATsnmQLcDxwAnBakiOBq4DD290/ATwW+B7wK+DZw4pLkqQ+GlpCr6qnzbDpkGn2LeAFw4pFkqS+Wy6d4iRJ0iKY0CVJ6gETuiRJPWBClySpB0zo05hYt4GJdRvGHYYkSZ2Z0CVJ6gETuiRJPWBClySpB0zokiT1gAldkqQeMKFLktQDJnRJknrAhC5JUg+Y0CVJ6gETuiRJPWBClySpB0zokiT1gAldkqQeMKFLktQDJnRJknrAhC5JUg+Y0CVJ6gETuiRJPWBClySpB0zokiT1gAldkqQeMKFLktQDJnRJknrAhC5JUg+Y0CVJ6gETuiRJPWBClySpB0zokiT1gAldkqQeGEtCT/KSJN9KcnGSDybZMcm+Sc5P8t0kH06ywzhikyRpJRp5Qk+yF3A0sKaq9ge2A54KvAF4S1WtBq4Djhx1bJIkrVTjanLfHtgpyfbAzsDVwMHA6e32U4FDxxSbJEkrzsgTelX9EDgRuIomkV8PbAZ+XlU3t7ttAfYadWySJK1U42hyvyPwRGBf4C7ALsBjptm1Zjj+qCSbkmzaunXr8AKVJGkFGUeT+yOBK6pqa1X9FjgDOBDYtW2CB9gb+NF0B1fV+qpaU1VrVq1aNZqIJUla5saR0K8CDkiyc5IAhwCXAOcCh7X7rAXOHENskiStSON4hn4+Tee3C4CL2hjWA68AXprke8CdgHeNOjZJklaq7efeZelV1fHA8VOKLwcePIZwJEla8RwpTpKkHjChS5LUAyZ0SZJ6wIQuSVIPmNBnMbFuAxPrNow7DEmS5mRClySpB0zokiT1gAldkqQeMKFLktQDcyb0JA9Nsku7/PQkb05yt+GHJkmSuupSQz8J+FWS+wLHAd8H3jvUqCRJ0rx0Seg3V1XRzGH+tqp6G3C74YYlSZLmo8vkLDcmeSXwdODhSbYDbj3csCRJ0nx0qaE/BfgNcGRV/RjYC3jjUKOSJEnz0qWG/pKqesXkSlVdleQ/DTEmSZI0T11q6I+apuwxSx2IJElauBlr6En+Bng+sF+Sbw5suh3w5WEHJkmSuputyf0DwCeBvwfWDZTfWFU/G2pUkiRpXmZM6FV1PXA98LS2Z/se7f63TXLbqrpqRDFKkqQ5zNkpLskLgdcA1wC3tMUF3Gd4YUmSpPno0sv9GOAeVfXTYQcjSZIWpksv9x/QNL1LkqRlqksN/XLgvCQbaAaYAaCq3jy0qCRJ0rx0SehXta8d2ldvTazbMO4QJElakDkTelW9FiDJLlX1y+GHJEmS5qvLfOgPSXIJcGm7ft8kbx96ZJIkqbMuneLeCvwF8FOAqvoG8PBhBiVJkuanS0Knqn4wpeh3Q4hFkiQtUJdOcT9IciBQSXYAjqZtfpckSctDlxr684AX0MyDvgW4X7suSZKWiS693H8CHDGCWCRJ0gLNNn3qcVX1D0n+iWbs9j9QVUcPNTJJktTZbDX0yefkm0YRiCRJWrjZpk89u30/dXThSJKkhZityf1spmlqn1RVT1joRZPsCpwM7N9e4znAZcCHgQngSuDJVXXdQq8hSdK2ZLZe7icCbwKuAH4NvLN9/QK4eJHXfRtwTlXdE7gvTfP+OmBjVa0GNrbrkiSpg9ma3D8HkOR/VdXgyHBnJ/n8Qi+Y5PY0I809q73OTcBNSZ4IHNTudipwHvCKhV5nKU1O2nLlCY8bcySSJE2vy/fQVyXZb3Ilyb7AqkVccz9gK/CeJBcmOTnJLsAeVXU1QPt+50VcQ5KkbUqXkeJeQjMf+uXt+gTw14u85gOAF1XV+Unexjya15McBRwFsM8++ywiDEmS+qPLwDLnJFkN3LMt+nZV/WYR19wCbKmq89v102kS+jVJ9qyqq5PsCVw7QzzrgfUAa9asmbHTniRJ25Iu06fuDBwLvLCdaW2fJI9f6AWr6sc048Pfoy06BLgEOAtY25atBc5c6DUkSdrWdGlyfw+wGXhIu74F+Ajw8UVc90XA+9vJXi4Hnk3zx8VpSY4ErgIOX8T5JUnapnRJ6HevqqckeRpAVf06SRZz0ar6OrBmmk2HLOa8kiRtq7r0cr8pyU60g8wkuTuwmGfokiRpiXWpoR8PnAPcNcn7gYfSfodckiQtD7Mm9LZp/dvAk4ADgAAvbqdUlSRJy8SsCb2qKsnHquqBwIYRxSRJkuapyzP0ryZ50NAjkSRJC9blGfojgL9O8n3glzTN7lVV9xlqZJIkqbMuCf0xQ49CkiQtSpeEfmPHMkmSNCZdnqFfQDM72neA77bLVyS5IMkDhxmcJEnqpktCPwd4bFXtXlV3ommCPw14PvD2YQYnSZK66ZLQ11TVpyZXqurTwMOr6qvAbYYWmSRJ6qzLM/SfJXkF8KF2/SnAdUm2A24ZWmSSJKmzLjX0/w7sDXyMZkrTfdqy7YAnDy80SZLU1Zw19HaY1xfNsPl7SxuOJElaiDkTepI/BV4OTAzuX1UHDy8sSZI0H12eoX8EeAdwMvC74YYjSZIWoktCv7mqThp6JJIkacG6dIo7O8nzk+yZZLfJ19AjkyRJnXWpoa9t348dKCtgv6UPR5IkLUSXXu77jiIQSZK0cDMm9CQHV9Vnkzxpuu1VdcbwwpIkSfMxWw39z4HPAn85zbYCTOiSJC0TMyb0qjq+fX/26MKRJEkL0aWXuyRJWuZM6JIk9cCMCT3J4e27vdwlSVrmZquhv7J9/+goApEkSQs3Wy/3nyY5F9g3yVlTN1bVE4YXliRJmo/ZEvrjgAcA7wPeNJpwJEnSQsz2tbWbgK8mObCqtia5XVNcvxhdeJIkqYsuvdz3SHIhcDFwSZLNSfYfclySJGkeuiT09cBLq+puVbUP8LK2TJIkLRNdEvouVXXu5EpVnQfsMrSIJEnSvHWZPvXyJH9H0zkO4OnAFcMLSZIkzVeXGvpzgFU0k7GcAewOOL67JEnLSJf50K8Djl7qCyfZDtgE/LCqHt+OSPchYDfgAuAZbU97SZI0h3GO5f5i4NKB9TcAb6mq1cB1wJFjiUqSpBVoLAk9yd40A9ec3K4HOBg4vd3lVODQccQmSdJKNGdCT/LQLmXz9FbgOOCWdv1OwM+r6uZ2fQuw1yKvIUnSNqNLDf2fOpZ1kuTxwLVVtXmweJpda4bjj0qyKcmmrVu3LjQMSZJ6ZcZOcUkeAhwIrEry0oFNtwe2W8Q1Hwo8IcljgR3b870V2DXJ9m0tfW/gR9MdXFXraQe2WbNmzbRJX5Kkbc1svdx3AG7b7nO7gfIbgMMWesGqeiXt1KxJDgJeXlVHJPlIe94PAWuBMxd6jWGZWLfhP5avPOFxY4xEkqQ/NNvkLJ8DPpfklKr6/ghieQXwoSSvAy4E3jWCa0qS1AtdRoq7TZL1wMTg/lV18GIv3g4je167fDnw4MWeU5KkbVGXhP4R4B00XzH73XDDkSRJC9Elod9cVScNPRJJkrRgXb62dnaS5yfZM8luk6+hRyZJkjrrUkNf274fO1BWwH5LH44kSVqILpOz7DuKQCRJ0sLNmdCTPHO68qp679KHI0mSFqJLk/uDBpZ3BA6hmd7UhC5J0jLRpcn9RYPrSe4AvG9oEUmSpHlbyPSpvwJWL3UgkiRp4bo8Qz+b3898th1wL+C0YQYlSZLmp8sz9BMHlm8Gvl9VW4YUjyRJWoA5m9zbSVq+TTPj2h2Bm4YdlCRJmp85E3qSJwP/BhwOPBk4P8mCp0+VJElLr0uT+98CD6qqawGSrAL+L3D6MAOTJEnddenlfqvJZN76acfjJEnSiHSpoZ+T5FPAB9v1pwCfHF5IkiRpvroMLHNskicBDwMCrK+qfx16ZJIkqbMZE3qSPwH2qKovVdUZwBlt+cOT3L2q/n1UQUqSpNnN9iz8rcCN05T/qt0mSZKWidkS+kRVfXNqYVVtAiaGFpEkSZq32RL6jrNs22mpA1lpJtZtYGLdhnGHIUkSMHtC/1qSv5pamORIYPPwQpIkSfM1Wy/3Y4B/TXIEv0/ga4AdgP867MAkSVJ3Myb0qroGODDJI4D92+INVfXZkUS2Qk02w195wuPGHIkkaVvS5Xvo5wLnjiAWSZK0QA7hKklSD5jQJUnqARP6Ivn1NUnScmBClySpB0zoQ2LNXZI0SiZ0SZJ6wIQuSVIPmNAlSeqBOQeWUTczPS935DhJ0iiMvIae5K5Jzk1yaZJvJXlxW75bks8k+W77fsdRxyZJ0ko1jib3m4GXVdW9gAOAFyS5N7AO2FhVq4GN7bokSepg5Am9qq6uqgva5RuBS4G9gCcCp7a7nQocOurYJElaqcbaKS7JBHB/4Hxgj6q6GpqkD9x5fJFJkrSyjC2hJ7kt8FHgmKq6YR7HHZVkU5JNW7duHV6AkiStIGNJ6EluTZPM319VZ7TF1yTZs92+J3DtdMdW1fqqWlNVa1atWjWagCVJWubG0cs9wLuAS6vqzQObzgLWtstrgTNHHZskSSvVOL6H/lDgGcBFSb7elr0KOAE4LcmRwFXA4WOITZKkFWnkCb2qvghkhs2HjDKW5cQBaCRJi+HQr5Ik9YAJfUScTlWSNEwmdEmSesCELklSD5jQJUnqARO6JEk9YEKXJKkHTOiSJPWACV2SpB4woUuS1AMmdEmSesCELklSD5jQlzmHjJUkdWFClySpB0zoI7bYGrc1dknSdEzokiT1gAldkqQeMKFLktQD2487gG3VXM/BfU4uSZoPa+iSJPWACV2SpB6wyX2ZsaldkrQQ1tAlSeoBE3rPOPCMJG2bTOiSJPWACX2Fm2+N3Bq8JPWTCV2SpB6wl/sKsdha9VIdf+UJj1vUeUZ5jVHELEnLhTV0SZJ6wBr6CtV16NiF1k6HWbud77mtaUvS3KyhS5LUA9bQe2Khz8inHjeMWvNM+4665r2cWga6nnuhv58+sqVGmp01dEmSesAaes/Nt+Y+df/5TPM6WXOa6Zhhf/99oTXwSYvtb7DY88znWjNdY65Ylqq1xNqytPwsqxp6kkcnuSzJ95KsG3c8kiStFMsmoSfZDvjfwGOAewNPS3Lv8UYlSdLKsJya3B8MfK+qLgdI8iHgicAlY41Kf2C2ZvOlalLvep6l6gg4rOO7PI5Yqk6Jcz3m6NpEP9/959o+U9N+l32G/ZXLhV5nqeKbtJDzjLqjadfPhY9sfm8cP+OyqaEDewE/GFjf0pZJkqQ5pKrGHQMASQ4H/qKqntuuPwN4cFW9aMp+RwFHtav3AC5bwjB2B36yhOfT3Lzn4+F9Hz3v+ej18Z7frapWTbdhOTW5bwHuOrC+N/CjqTtV1Xpg/TACSLKpqtYM49yanvd8PLzvo+c9H71t7Z4vpyb3rwGrk+ybZAfgqcBZY45JkqQVYdnU0Kvq5iQvBD4FbAe8u6q+NeawJElaEZZNQgeoqk8AnxhjCENpytesvOfj4X0fPe/56G1T93zZdIqTJEkLt5yeoUuSpAUyoeOQs6OS5K5Jzk1yaZJvJXlxW75bks8k+W77fsdxx9o3SbZLcmGSj7fr+yY5v73nH247omqJJNk1yelJvt1+3h/i53z4kryk/b/l4iQfTLLjtvRZ3+YTukPOjtTNwMuq6l7AAcAL2nu9DthYVauBje26ltaLgUsH1t8AvKW959cBR44lqv56G3BOVd0TuC/NvfdzPkRJ9gKOBtZU1f40naufyjb0Wd/mEzoDQ85W1U3A5JCzWmJVdXVVXdAu30jzn9xeNPf71Ha3U4FDxxNhPyXZG3gccHK7HuBg4PR2F+/5Ekpye+DhwLsAquqmqvo5fs5HYXtgpyTbAzsDV7MNfdZN6A45OxZJJoD7A+cDe1TV1dAkfeDO44usl94KHAfc0q7fCfh5Vd3crvuZX1r7AVuB97SPOU5Osgt+zoeqqn4InAhcRZPIrwc2sw191k3okGnK7Po/REluC3wUOKaqbhh3PH2W5PHAtVW1ebB4ml39zC+d7YEHACdV1f2BX2Lz+tC1fRKeCOwL3AXYheZR6lS9/ayb0DsOOaulkeTWNMn8/VV1Rlt8TZI92+17AteOK74eeijwhCRX0jxOOpimxr5r2ywJfuaX2hZgS1Wd366fTpPg/ZwP1yOBK6pqa1X9FjgDOJBt6LNuQnfI2ZFpn92+C7i0qt48sOksYG27vBY4c9Sx9VVVvbKq9q6qCZrP9mer6gjgXOCwdjfv+RKqqh8DP0hyj7boEJppoP2cD9dVwAFJdm7/r5m879vMZ92BZYAkj6WptUwOOfv6MYfUS0keBnwBuIjfP899Fc1z9NOAfWj+UR5eVT8bS5A9luQg4OVV9fgk+9HU2HcDLgSeXlW/GWd8fZLkfjSdEHcALgeeTVOB8nM+REleCzyF5hs1FwLPpXlmvk181k3okiT1gE3ukiT1gAldkqQeMKFLktQDJnRJknrAhC5JUg+Y0NUbSSrJmwbWX57kNXMcc+iwJuNJckySnefY5zVJXr6IazwryT9PU/6E+cwcmGRVki+2s1QdOlB+ZpK7zDOmVe3sVhcm+S9Ttp08eb+TvGo+55U0OxO6+uQ3wJOS7D6PYw6lmWVvGI6hmSBi5KrqrKo6YR6HPI1m4oqHAMcCJPlL4IKqmu/IWocA366q+1fVF6bE9dyquqRdXZEJvZ2hUVp2TOjqk5uB9cBLpm5IcrckG5N8s33fJ8mBwBOANyb5epK7TznmlCQntXO4X57kz5O8u53f+pSB/U5Ksqmdh/m1bdnRNONJn5vk3Lbs0UkuSPKNJBsHLnXvJOe11zh64LxPT/JvbWz/MplIkjw7yXeSfI5maNc/Mlhzb3+Of0zy5fYah01zyG+BnYDbALe0Q2UeA7xxpps9wz29H/APwGPbuHeacsx5SdYkOYFmVqyvJ3l/kl2SbGjvzcVJnjLN9f4qydfafT462frR/nzvSPKF9r48fuAenJnknCSXJTm+w739o99lW35lklcn+SJw+ByxTHuvkxyX5KL2mBPasru38W1u479nW354ex++keTzM/0OpD9QVb589eIF/AK4PXAlcAfg5cBr2m1nA2vb5ecAH2uXTwEOm+F8p9CMMBWaSR9uAP4zzR/Cm4H7tfvt1r5vB5wH3KddvxLYvV1eRTOr375TjnkN8GWaRLo78FPg1sC92phv3e73duCZwJ40o4ytohmF7EvAP08T+7Mmy9uf4yNt3PemmS546v53ADYAm2hq2EdP3q9Z7vdM9/RZ08XUbjuPZr5qgF8MlP834J2D8Uxz7J0Gll8HvGjg5zun/flW04ylvmMbx9U0s8vtBFwMrJnp3nb4XR7XMZY/utc0k4R8Gdh5ynU2Aqvb5T+jGZoXmtEU92qXdx33vy1fK+M1OWC91AtVdUOS99IkpF8PbHoI8KR2+X00tcguzq6qSnIRcE1VXQSQ5FvABPB14MlJjqKZZWtPmv/IvznlPAcAn6+qK9o4B4f83FDNUJS/SXItsAdNUn0g8LUk0CSka2n+0z+vqra2cXwY+NMOP8fHquoW4JIke0zdWFXX08yZPjlr1StoHl+8E7gj8Kaq+sqUwxZ6T6dzEXBikjcAH68pTfWt/ZO8DtgVuC3wqYFtp7U/33eTXA7csy3/TFX9tP25zgAeRtOSM929hdl/lx/uGMt09/qRwHuq6lfQ/P7TzDp4IPCRNg5o/rCD5g+1U5KcRjPJiDQnE7r66K3ABcB7Ztmn65jHk2M+3zKwPLm+fZJ9aVoCHlRV17VN8TtOc57Mcs3B8/6O5t9lgFOr6pV/cJKmw9pCxmsevMZ006cOejXweprn6puBD9BMaPGIOY5b8DjSVfWdJA8EHgv8fZJPV9X/nLLbKcChVfWNJM8CDprl2jVL+Uz3dq7f5S87xjLdvZ7u938rmrm67zelnKp6XpI/o/kj6+tJ7jf5h4k0E5+hq3fa2u9pwJEDxV+mmW0M4Ajgi+3yjcDtFnG529P8R399WxsbnH958NxfAf68TRok2W2O824EDkty58n9k9yNZiKbg5LcKc1UtIcvIvY/kmQ1cJeq+hxNh75baBLRdH+kzHRPu/pt+zOQpif9r6rq/wAn0kw3OtXtgKvbY46Ysu3wJLdK0w9iP+CytvxR7b3biaYD5JeY+d7O9rucTyzT+TTwnIFn7btV1Q3AFUkOb8uS5L7t8t2r6vyqejXwE/5wimdpWtbQ1VdvAl44sH408O4kxwJbaWa/guYZ+TvTdEY7rKr+fT4XaWtoFwLfoplV60sDm9cDn0xydVU9om3KPSPJrWiaeB81y3kvSfI/gE+3+/8WeEFVfTXNV/G+QvN8+AKa571L5fXA37bLHwQ+BryYptY+1Uz3tKv1wDeTXAC8l6Zz4i00P+vfTLP/39H8QfN9mib6wT/ELgM+R/O44nlV9f/aZuwv0jwO+BPgA1W1CWCWezvT73I+sfyRqjonTYfBTUluAj5B08v/COCkNp5b03wev9Hei9U0NfuNbZk0K2dbk7SitU3jH6+q06eUP4umA94LpztO6hub3CVJ6gFr6JIk9YA1dEmSesCELklSD5jQJUnqARO6JEk9YEKXJKkHTOiSJPXA/wfmrCQGCeqTKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "missing_in_num_recipes = [(col, col_sum) for col, col_sum in missing_matrix.sum().iteritems()]\n",
    "missing_pct = [(col, (col_sum / dict_cluster_count[col])*100) for (col, col_sum) in missing_in_num_recipes]\n",
    "\n",
    "thres_pct = 20\n",
    "print('There are', len(no_match_list), 'recipes in total.')\n",
    "print('There are', len(missing_matrix), 'recipes with at least 1 ingredient not matched.')\n",
    "print('There are', len(all_no_match_unique), 'unique ingredients not matched in at least 1 recipe out of',len(no_match_list),'recipes.')\n",
    "print('There are', len([col for col, pct in missing_pct if pct <= thres_pct]), 'unique ingredients not matched in <=', thres_pct,'% of its appearances.')\n",
    "print('There are', len(all_no_match_unique) - len([col for col, pct in missing_pct if pct <= thres_pct]), 'unique ingredients not matched in >', thres_pct,'% of its appearances.')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "plt.hist([pct for col, pct in missing_pct], density=False, bins=200)\n",
    "plt.ylabel('Count of ingredients')\n",
    "plt.xlabel('Not matched in % of its appearances') ### change to pct, not matched / all it appears， sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [col for col, pct in missing_pct if pct > thres_pct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 87)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the ingredients that only not matched in <= thres recipes\n",
    "# drop the recipes that contain more than 1 missing ingredient now\n",
    "\n",
    "missing_matrix_drop = missing_matrix.copy()\n",
    "\n",
    "missing_matrix_drop.drop([col for col, pct in missing_pct if pct <= thres_pct], axis=1, inplace=True)\n",
    "missing_matrix_drop = missing_matrix_drop.loc[(missing_matrix_drop.sum(axis=1) > 1),:]\n",
    "missing_matrix_drop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = missing_matrix_drop.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [15, 15]\n",
    "# sn.heatmap(corr_matrix, annot=True, cmap=\"Blues\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_lower = corr_matrix.copy()\n",
    "\n",
    "corr_matrix_lower.loc[:,:] =  np.tril(corr_matrix_lower, k=-1) # borrowed from Karl D's answer\n",
    "\n",
    "already_in = set()\n",
    "missing_ingredients_clusters = []\n",
    "for col in corr_matrix_lower:\n",
    "    high_corr = corr_matrix_lower[col][corr_matrix_lower[col] > 0.02].index.tolist()\n",
    "    if high_corr and col not in already_in:\n",
    "        already_in.update(set(high_corr))\n",
    "        high_corr.append(col)\n",
    "        missing_ingredients_clusters.append(high_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the direction texts missing the above clusters of ingredients\n",
    "\n",
    "# no_match_df = matched_df.loc[matched_df['num_no_match']>0,:]\n",
    "\n",
    "# check_list = no_match_df.loc[no_match_df.no_match.map(lambda x: 'baking_soda' in x), 'direction']\n",
    "# for direction in check_list:\n",
    "#     print(direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dict containing all the mappings from above findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dict for cluster name mapping\n",
    "\n",
    "key: cluster name\n",
    "value: dict{'short': all appeared short forms (only for multi-gram cluster names), \n",
    "            'parent': parent names, \n",
    "            'child': child names,\n",
    "            'synonym': synonyms}\n",
    "'''\n",
    "\n",
    "all_short_forms = []\n",
    "dict_ingre_mapping = defaultdict(lambda: defaultdict(list))\n",
    "for partial_match_this_recipe in partial_match_list:\n",
    "    for partial_match_this_ingre in partial_match_this_recipe:\n",
    "        ingre = partial_match_this_ingre[0]\n",
    "        \n",
    "        # add \"dry ingredient\" as parent term for ingredients with \"dried\"\n",
    "        if ('dried' in ingre) and ('dry_ingredient' not in dict_ingre_mapping[ingre]['parent']):\n",
    "            dict_ingre_mapping[ingre]['parent'].append('dry_ingredient')\n",
    "        \n",
    "        # add parent names for ingredients with \"broth\"\n",
    "        if ('broth' in ingre) and ('stock' not in dict_ingre_mapping[ingre]['parent']):\n",
    "            dict_ingre_mapping[ingre]['parent'] += ['stock','consomme','soup']\n",
    "        short_forms = partial_match_this_ingre[1]\n",
    "        \n",
    "        # add parent names for ingredients with \"preserve\"\n",
    "        if ('jam' in ingre) and ('preserve' not in dict_ingre_mapping[ingre]['parent']):\n",
    "            dict_ingre_mapping[ingre]['parent'] += ['preserve']\n",
    "        short_forms = partial_match_this_ingre[1]\n",
    "        \n",
    "        # add in all short forms to the dict\n",
    "        for short_form in short_forms:\n",
    "            if short_form not in dict_ingre_mapping[ingre]['short']:\n",
    "                dict_ingre_mapping[ingre]['short'].append(short_form)\n",
    "                \n",
    "                \n",
    "#### add in parent terms\n",
    "# nut\n",
    "nut_cluster = ['walnut', 'pecan', 'almond']\n",
    "for ingre in nut_cluster:\n",
    "    dict_ingre_mapping[ingre]['parent'].append('nut')\n",
    "\n",
    "# spice\n",
    "spice_cluster = ['ground_cinnamon', 'ground_nutmeg']\n",
    "for ingre in spice_cluster:\n",
    "    dict_ingre_mapping[ingre]['parent'].append('spice')\n",
    "\n",
    "# meat\n",
    "meat_cluster = ['ground_beef', 'pork_sausage']\n",
    "for ingre in meat_cluster:\n",
    "    dict_ingre_mapping[ingre]['parent'].append('meat')\n",
    "    \n",
    "# roast\n",
    "roast_cluster = ['beef_chuck', 'beef_tenderloin']\n",
    "for ingre in roast_cluster:\n",
    "    dict_ingre_mapping[ingre]['parent'].append('roast')\n",
    "\n",
    "# steak\n",
    "steak_cluster = ['beef', 'beef_sirloin', 'pork']\n",
    "for ingre in steak_cluster:\n",
    "    dict_ingre_mapping[ingre]['parent'].append('steak')\n",
    "    \n",
    "# berry\n",
    "berry_cluster = ['blackberry','blueberry']\n",
    "for ingre in berry_cluster:\n",
    "    dict_ingre_mapping[ingre]['parent'].append('berry')\n",
    "\n",
    "\n",
    "dict_ingre_mapping['rose']['parent'].append('petal')\n",
    "dict_ingre_mapping['graham_cracker']['parent'] += ['crust', 'crumb']\n",
    "dict_ingre_mapping['ground_beef']['parent'] += ['meatloaf', 'patty']\n",
    "dict_ingre_mapping['sirloin']['parent'].append('beef')\n",
    "dict_ingre_mapping['beef']['parent'] += ['rib', 'tamale']\n",
    "dict_ingre_mapping['cod']['parent'] += ['fish', 'fillet']\n",
    "\n",
    "\n",
    "\n",
    "#### add in child terms\n",
    "dict_ingre_mapping['topping']['child'].append('spice')\n",
    "dict_ingre_mapping['potato']['child'].append('frenchfry')\n",
    "dict_ingre_mapping[\"pig's_part\"]['child'] += ['pork', 'liver', 'ear', 'tail', 'foot', 'pig', 'jowl', 'stomach', 'cheek']\n",
    "dict_ingre_mapping[\"liqueur\"]['child'] += ['daiquiri','cocktail', 'chartreuse', 'cachaca', 'mezcal']\n",
    "dict_ingre_mapping[\"bean\"]['child'] += ['soybean']\n",
    "dict_ingre_mapping[\"other_fish\"]['child'] += ['bonito', 'fillet', 'milkfish', 'herring', 'mackerel', \n",
    "                                              'sardine', 'bluefish', 'perch', 'monkfish', 'bass', \n",
    "                                             'hake', 'amberjack', 'butterfish', 'pollock', 'rockfish', \n",
    "                                              'pompano', 'milkfish', 'char', 'smelt', 'mullet', 'saury']\n",
    "\n",
    "\n",
    "#### add in synonyms\n",
    "dict_ingre_mapping['garbanzo_bean']['synonym'].append('chickpea')\n",
    "dict_ingre_mapping['green_onion']['synonym'].append('scallion')\n",
    "dict_ingre_mapping['espresso']['synonym'].append('coffee_bean')\n",
    "# dict_ingre_mapping['margarine']['synonym'].append('butter')\n",
    "# dict_ingre_mapping['butter']['synonym'].append('margarine')\n",
    "dict_ingre_mapping['crabmeat']['synonym'].append('crab')\n",
    "dict_ingre_mapping['green_chile_pepper']['synonym'].append('chili')\n",
    "dict_ingre_mapping['spaghetti']['synonym'] += ['spaghettini', 'pasta']\n",
    "dict_ingre_mapping['macaroni']['synonym'] += ['pasta']\n",
    "dict_ingre_mapping['pasta']['synonym'] += ['rotelle', 'cavatappi']\n",
    "dict_ingre_mapping['bread_crumb']['synonym'] += ['panko']\n",
    "dict_ingre_mapping['raisin']['synonym'] += ['sultana']\n",
    "dict_ingre_mapping['adobo_sauce']['synonym'] += ['chipotle']\n",
    "dict_ingre_mapping['sweetener']['synonym'] += ['fructose']\n",
    "dict_ingre_mapping['bread']['synonym'] += ['loaf', 'toast', 'challah', 'farl']\n",
    "dict_ingre_mapping['cornflakes_cereal']['synonym'] += ['cornflake']\n",
    "dict_ingre_mapping['parsley']['synonym'] += ['chervil']\n",
    "dict_ingre_mapping['cod']['synonym'] += ['lingcod']\n",
    "dict_ingre_mapping['soy_sauce']['synonym'] += ['tamari']\n",
    "dict_ingre_mapping['orange_jam']['synonym'] += ['marmalade']\n",
    "dict_ingre_mapping['pie_crust']['synonym'] += ['shell']\n",
    "dict_ingre_mapping['green_tea']['synonym'] += ['matcha']\n",
    "dict_ingre_mapping['beef_tenderloin']['synonym'] += ['mignon_filet', 'filet', 'mignon', 'steak']\n",
    "dict_ingre_mapping['beer']['synonym'] += ['stout', 'ale']\n",
    "dict_ingre_mapping['buttermilk']['synonym'] += ['sour_milk']\n",
    "dict_ingre_mapping['club_soda']['synonym'] += ['carbonated_water', 'seltzer', 'sparkling_water']\n",
    "dict_ingre_mapping['cooky']['synonym'] += ['biscuit']\n",
    "dict_ingre_mapping['nori']['synonym'] += ['seaweed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dict\n",
    "# for key in dict_ingre_mapping:\n",
    "#     print('\\n',key)\n",
    "#     print(dict_ingre_mapping[key])\n",
    "\n",
    "# Save the dict\n",
    "\n",
    "# import csv\n",
    "\n",
    "# with open('/Users/nessyliu/Desktop/RA/part_2/result/dict_ingre_mapping_w5.csv', 'w') as f:\n",
    "#     for key in dict_ingre_mapping.keys():\n",
    "#         f.write(\"%s,%s\\n\"%(key,dict_ingre_mapping[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3dbde2708d414484c83eebf3792119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=57709.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "matched_df2 = pd.read_csv('/Users/nessyliu/Desktop/RA/part_2/result/matched_directions_orgin_w5.csv')\n",
    "\n",
    "#print(matched_df2.dtypes)\n",
    "dict_match_list = []\n",
    "num_dict_match_list = []\n",
    "num_ignore_list = []\n",
    "new_no_match_list = []\n",
    "\n",
    "for i in tqdm(range(0,len(matched_df2))):\n",
    "    row = matched_df2.iloc[i,:]\n",
    "    dict_match_this_recipe = [] # match of missing ingredients in this recipe by using the dict\n",
    "    num_dict_match_this_recipe = 0\n",
    "    num_ignore = 0\n",
    "    no_match_this_recipe = literal_eval(row.no_match)\n",
    "    if row.num_no_match > 0:\n",
    "        direction = row.direction + ' ' + row.lemma_direction\n",
    "        for missing_ingre in no_match_this_recipe:\n",
    "            if missing_ingre in ['salt', 'water']:\n",
    "                num_ignore += 1\n",
    "            else:\n",
    "                dict_match_this_ingre = []\n",
    "                parent = dict_ingre_mapping[missing_ingre]['parent']\n",
    "                short = dict_ingre_mapping[missing_ingre]['short']\n",
    "                child = dict_ingre_mapping[missing_ingre]['child']\n",
    "                synonym = dict_ingre_mapping[missing_ingre]['synonym']\n",
    "                all_possible_names = synonym + parent + short + child\n",
    "                for name in all_possible_names:\n",
    "                    if name.replace('_', ' ') in direction: # if there is a match between the missing ingre and name in the dict\n",
    "                        dict_match_this_ingre.append(name)\n",
    "\n",
    "                if len(dict_match_this_ingre) > 0:\n",
    "                    dict_match_this_recipe.append([missing_ingre, dict_match_this_ingre])\n",
    "                    num_dict_match_this_recipe += 1\n",
    "                    # if this ingre is matched using dict, remove it from no_match column, and modify the num_no_match\n",
    "                    no_match_this_recipe.remove(missing_ingre)\n",
    "                    matched_df2.loc[i,'num_no_match'] -= 1\n",
    "\n",
    "    dict_match_list.append(dict_match_this_recipe)\n",
    "    num_dict_match_list.append(num_dict_match_this_recipe)\n",
    "    num_ignore_list.append(num_ignore)\n",
    "    new_no_match_list.append(no_match_this_recipe)\n",
    "\n",
    "    \n",
    "matched_df2 = matched_df2.drop(['no_match'], axis = 1)\n",
    "matched_df2.insert(7, 'dict_match', dict_match_list)\n",
    "matched_df2.insert(8, 'no_match', new_no_match_list)\n",
    "matched_df2.insert(12, 'num_dict_match', num_dict_match_list)\n",
    "matched_df2.insert(14, 'num_ignore', num_ignore_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df2.to_csv('/Users/nessyliu/Desktop/RA/part_2/result/matched_directions_w5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ingredients in all recipes:\n",
      "510860\n",
      "Number of missing ingredients after perfect match:\n",
      "155728\n",
      "Number of missing ingredients after partial match:\n",
      "12182 2.38%\n",
      "Number of missing ingredients after using our mapping dict:\n",
      "9307 1.82%\n",
      "Number of missing ingredients after removing 'salt', 'water':\n",
      "8511 1.67%\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of ingredients in all recipes:\")\n",
    "print(sum(matched_df.num_ingredient))\n",
    "print(\"Number of missing ingredients after perfect match:\")\n",
    "print(sum(matched_df.num_ingredient) - sum(matched_df.num_perfect_match))\n",
    "print(\"Number of missing ingredients after partial match:\")\n",
    "print(sum(matched_df.num_no_match), \"{0:.2%}\".format(sum(matched_df.num_no_match) / sum(matched_df2.num_ingredient)))\n",
    "print(\"Number of missing ingredients after using our mapping dict:\")\n",
    "print(sum(matched_df2.num_no_match), \"{0:.2%}\".format(sum(matched_df2.num_no_match) / sum(matched_df2.num_ingredient)))\n",
    "print(\"Number of missing ingredients after removing 'salt', 'water':\")\n",
    "print(sum(matched_df2.num_no_match) - sum(matched_df2.num_ignore), \"{0:.2%}\".format((sum(matched_df2.num_no_match) - sum(matched_df2.num_ignore)) / sum(matched_df2.num_ingredient)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_in_num_recipes = [(col, col_sum) for col, col_sum in missing_matrix.sum().iteritems()]\n",
    "# missing_pct = [(col, (col_sum / dict_cluster_count[col])*100) for (col, col_sum) in missing_in_num_recipes]\n",
    "\n",
    "# thres_pct = 20\n",
    "# print('There are', len(no_match_list), 'recipes in total.')\n",
    "# print('There are', len(missing_matrix), 'recipes with at least 1 ingredient not matched.')\n",
    "# print('There are', len(all_no_match_unique), 'unique ingredients not matched in at least 1 recipe out of',len(no_match_list),'recipes.')\n",
    "# print('There are', len([col for col, pct in missing_pct if pct <= thres_pct]), 'unique ingredients not matched in <=', thres_pct,'% of its appearances.')\n",
    "# print('There are', len(all_no_match_unique) - len([col for col, pct in missing_pct if pct <= thres_pct]), 'unique ingredients not matched in >', thres_pct,'% of its appearances.')\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "# plt.hist([pct for col, pct in missing_pct], density=False, bins=200)\n",
    "# plt.ylabel('Count of ingredients')\n",
    "# plt.xlabel('Not matched in % of its appearances') ### change to pct, not matched / all it appears， sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>recipe_url</th>\n",
       "      <th>raw_id</th>\n",
       "      <th>ingredient_text</th>\n",
       "      <th>ingredient_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77367</th>\n",
       "      <td>24170</td>\n",
       "      <td>https://www.allrecipes.com/recipe/24170/mexica...</td>\n",
       "      <td>5133</td>\n",
       "      <td>1 (6 ounce) can chopped black olives, drained</td>\n",
       "      <td>chopped black olive drained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132523</th>\n",
       "      <td>24170</td>\n",
       "      <td>https://www.allrecipes.com/recipe/24170/mexica...</td>\n",
       "      <td>4405</td>\n",
       "      <td>1 green onion, chopped</td>\n",
       "      <td>green onion chopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234144</th>\n",
       "      <td>24170</td>\n",
       "      <td>https://www.allrecipes.com/recipe/24170/mexica...</td>\n",
       "      <td>2820</td>\n",
       "      <td>2 (15 ounce) cans chili</td>\n",
       "      <td>chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537487</th>\n",
       "      <td>24170</td>\n",
       "      <td>https://www.allrecipes.com/recipe/24170/mexica...</td>\n",
       "      <td>16223</td>\n",
       "      <td>2 (8 ounce) packages cream cheese, softened</td>\n",
       "      <td>cream cheese softened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541172</th>\n",
       "      <td>24170</td>\n",
       "      <td>https://www.allrecipes.com/recipe/24170/mexica...</td>\n",
       "      <td>16215</td>\n",
       "      <td>1 (16 ounce) package shredded Cheddar cheese</td>\n",
       "      <td>shredded cheddar cheese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        recipe_id                                         recipe_url  raw_id  \\\n",
       "77367       24170  https://www.allrecipes.com/recipe/24170/mexica...    5133   \n",
       "132523      24170  https://www.allrecipes.com/recipe/24170/mexica...    4405   \n",
       "234144      24170  https://www.allrecipes.com/recipe/24170/mexica...    2820   \n",
       "537487      24170  https://www.allrecipes.com/recipe/24170/mexica...   16223   \n",
       "541172      24170  https://www.allrecipes.com/recipe/24170/mexica...   16215   \n",
       "\n",
       "                                      ingredient_text  \\\n",
       "77367   1 (6 ounce) can chopped black olives, drained   \n",
       "132523                         1 green onion, chopped   \n",
       "234144                        2 (15 ounce) cans chili   \n",
       "537487    2 (8 ounce) packages cream cheese, softened   \n",
       "541172   1 (16 ounce) package shredded Cheddar cheese   \n",
       "\n",
       "              ingredient_clean_text  \n",
       "77367   chopped black olive drained  \n",
       "132523          green onion chopped  \n",
       "234144                        chili  \n",
       "537487        cream cheese softened  \n",
       "541172      shredded cheddar cheese  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 25510,24803,24632,\n",
    "df_ingre_clean.loc[df_ingre_clean['recipe_id'].isin([24170]), :].sort_values(['recipe_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>ingredient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164740</th>\n",
       "      <td>24170</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164741</th>\n",
       "      <td>24170</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164742</th>\n",
       "      <td>24170</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164743</th>\n",
       "      <td>24170</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164744</th>\n",
       "      <td>24170</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164745</th>\n",
       "      <td>24170</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        recipe_id  ingredient_id\n",
       "164740      24170            115\n",
       "164741      24170            535\n",
       "164742      24170            256\n",
       "164743      24170             84\n",
       "164744      24170            352\n",
       "164745      24170            222"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ingredients_raw.loc[df_ingredients_raw['recipe_id'] == 24170,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bean'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ingredient_clustername[84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fcc1d38f1a4e9b85b87c642047bc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=57709.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "error_recipe_list = []\n",
    "for recipe_id in tqdm(matched_df2.recipe_id):\n",
    "    # number of ingre in ingredients_after_text_cleaning.csv\n",
    "    num_ingre_1 = len(df_ingre_clean.loc[df_ingre_clean['recipe_id'] == recipe_id, :])\n",
    "    # number of ingre in Ingredients.csv\n",
    "    num_ingre_2 = len(df_ingredients_raw.loc[df_ingredients_raw['recipe_id'] == recipe_id,:])\n",
    "    if num_ingre_1 != num_ingre_2:\n",
    "        error_recipe_list.append(recipe_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0787b1150f3148a3938b5828a9d72c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=57709.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Create dict to store the count of each mapping\n",
    "dict_mapping_count = copy.deepcopy(dict_ingre_mapping)\n",
    "\n",
    "for cluster, values in dict_mapping_count.items():\n",
    "    # initialize parent count\n",
    "    parent_list = values['parent']\n",
    "    values['parent'] = {}\n",
    "    for parent in parent_list:\n",
    "        values['parent'][parent] = 0\n",
    "    \n",
    "    # initialize child count\n",
    "    child_list = values['child']\n",
    "    values['child'] = {}\n",
    "    for child in child_list:\n",
    "        values['child'][child] = 0\n",
    "    \n",
    "    # initialize synonym count\n",
    "    synonym_list = values['synonym']\n",
    "    values['synonym'] = {}\n",
    "    for synonym in synonym_list:\n",
    "        values['synonym'][synonym] = 0\n",
    "    \n",
    "    # initialize short term count\n",
    "    short_list = values['short']\n",
    "    values['short'] = {}\n",
    "    for short in short_list:\n",
    "        values['short'][short] = 0\n",
    "    \n",
    "    # initialize full match count\n",
    "    values['full'] = 0\n",
    "    \n",
    "    # initialize no match count\n",
    "    values['no'] = 0\n",
    "\n",
    "\n",
    "for cluster_name,_ in dict_cluster_count.items():\n",
    "    if cluster_name not in dict_mapping_count.keys():\n",
    "        dict_mapping_count[cluster_name]['full'] = 0\n",
    "        dict_mapping_count[cluster_name]['no'] = 0\n",
    "        dict_mapping_count[cluster_name]['short'] = {}\n",
    "        dict_mapping_count[cluster_name]['parent'] = {}\n",
    "        dict_mapping_count[cluster_name]['synonym'] = {}\n",
    "        dict_mapping_count[cluster_name]['child'] = {}\n",
    "\n",
    "# calculate the counts\n",
    "for i in tqdm(range(0,len(matched_df2))):\n",
    "    row = matched_df2.iloc[i,:]\n",
    "    perfect_match = literal_eval(row.perfect_match)\n",
    "    partial_match = literal_eval(row.partial_match)\n",
    "    dict_match = row.dict_match\n",
    "    no_match = row.no_match\n",
    "    \n",
    "    # count full match\n",
    "    for match in perfect_match:\n",
    "        dict_mapping_count[match]['full'] += 1\n",
    "    \n",
    "    # count partial match\n",
    "    for match in partial_match:\n",
    "        cluster_name = match[0]\n",
    "        short_terms = match[1]\n",
    "        for short in short_terms:\n",
    "            dict_mapping_count[cluster_name]['short'][short] += 1\n",
    "    \n",
    "    # count parent, child, synonym match\n",
    "    for match in dict_match:\n",
    "        cluster_name = match[0]\n",
    "        matched_terms = match[1]\n",
    "        parents = dict_ingre_mapping[cluster_name]['parent']\n",
    "        childs = dict_ingre_mapping[cluster_name]['child']\n",
    "        synonyms = dict_ingre_mapping[cluster_name]['synonym']\n",
    "        for term in matched_terms:\n",
    "            if term in parents:\n",
    "                dict_mapping_count[cluster_name]['parent'][term] += 1\n",
    "            elif term in childs:\n",
    "                dict_mapping_count[cluster_name]['child'][term] += 1\n",
    "            elif term in synonyms:\n",
    "                dict_mapping_count[cluster_name]['synonym'][term] += 1\n",
    "            else:\n",
    "                print(\"error!\")\n",
    "    \n",
    "    # count no match\n",
    "    for cluster_name in no_match:\n",
    "        dict_mapping_count[cluster_name]['no'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df storing the counts in dict_mapping_count\n",
    "cluster_name_list = []\n",
    "cluster_count_list = []\n",
    "full_match_count_list = []\n",
    "no_match_count_list = []\n",
    "partial_match_count_list = []\n",
    "partial_match_total_list = []\n",
    "parent_match_count_list = []\n",
    "parent_match_total_list = []\n",
    "child_match_count_list = []\n",
    "child_match_total_list = []\n",
    "synonym_match_count_list = []\n",
    "synonym_match_total_list = []\n",
    "\n",
    "for cluster_name, values in dict_mapping_count.items():\n",
    "    cluster_name_list.append(cluster_name)\n",
    "    cluster_count_list.append(dict_cluster_count[cluster_name])\n",
    "    full_match_count_list.append(values['full'])\n",
    "    no_match_count_list.append(values['no'])\n",
    "    \n",
    "    partial_matches = values['short']\n",
    "    parent_matches = values['parent']\n",
    "    child_matches = values['child']\n",
    "    synonym_matches = values['synonym']\n",
    "    \n",
    "    partial_match_total = 0\n",
    "    parent_match_total = 0\n",
    "    child_match_total = 0\n",
    "    synonym_match_total = 0\n",
    "    \n",
    "    partial_matches_this_cluster = []\n",
    "    for term, value in partial_matches.items():\n",
    "        partial_matches_this_cluster.append([term, value])\n",
    "        partial_match_total += value\n",
    "        \n",
    "    parent_matches_this_cluster = []\n",
    "    for term, value in parent_matches.items():\n",
    "        parent_matches_this_cluster.append([term, value])\n",
    "        parent_match_total += value\n",
    "            \n",
    "    child_matches_this_cluster = []\n",
    "    for term, value in child_matches.items():\n",
    "        child_matches_this_cluster.append([term, value])\n",
    "        child_match_total += value\n",
    "    \n",
    "    synonym_matches_this_cluster = []\n",
    "    for term, value in synonym_matches.items():\n",
    "        synonym_matches_this_cluster.append([term, value])\n",
    "        synonym_match_total += value\n",
    "\n",
    "    partial_match_count_list.append(partial_matches_this_cluster)\n",
    "    parent_match_count_list.append(parent_matches_this_cluster)\n",
    "    child_match_count_list.append(child_matches_this_cluster)\n",
    "    synonym_match_count_list.append(synonym_matches_this_cluster)\n",
    "    \n",
    "    partial_match_total_list.append(partial_match_total)\n",
    "    parent_match_total_list.append(parent_match_total)\n",
    "    child_match_total_list.append(child_match_total)\n",
    "    synonym_match_total_list.append(synonym_match_total)\n",
    "\n",
    "df_stats = pd.DataFrame({\n",
    "    'cluster_name': cluster_name_list,\n",
    "    'num_of_recipes': cluster_count_list,\n",
    "    'full_match_stat': full_match_count_list,\n",
    "    'no_match_stat': no_match_count_list,\n",
    "    'partial_match': partial_match_count_list,\n",
    "    'partial_match_stat': partial_match_total_list,\n",
    "    'parent_match': parent_match_count_list,\n",
    "    'parent_match_stat': parent_match_total_list,\n",
    "    'child_match': child_match_count_list,\n",
    "    'child_match_stat': child_match_total_list,\n",
    "    'synonym_match': synonym_match_count_list,\n",
    "    'synonym_match_stat': synonym_match_total_list,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.to_csv('/Users/nessyliu/Desktop/RA/part_2/result/df_stats_w5.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
