{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import csv\n",
    "import itertools\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag, corpus\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict, Counter\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from ast import literal_eval\n",
    "import dill as pickle\n",
    "import more_itertools as mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for reading and saving Python objects\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open('OUTPUT_OF_FINAL_CODE/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('OUTPUT_OF_FINAL_CODE/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the prefix to be added to any detected ingredients\n",
    "prefix = 'INGRE_'\n",
    "\n",
    "# set the path to your tagger: nltk_data/taggers/maxent_treebank_pos_tagger/english.pickle\n",
    "# need to download the tagger (from nltk) if do not have\n",
    "path_to_nltk_tagger = \"/Users/nessyliu/nltk_data/taggers/maxent_treebank_pos_tagger/english.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data files and some data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets\n",
    "df = pd.read_csv('input_data/AllReviews_26thNov2019.csv')\n",
    "df_ingredients_raw = pd.read_csv('input_data/Ingredients.csv')\n",
    "df_cluster = pd.read_excel('input_data/Cluster_names.xlsx')\n",
    "\n",
    "# load the mapping dict\n",
    "dict_ingre_mapping = load_obj('dict_ingre_mapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of cluster names with spaces (e.g. apple juice) sorted by number of words in each name, \n",
    "# e.g. \"apple juice\" should appear before \"juice\"\n",
    "cluster_name_orig_list = df_cluster.cluster_name.tolist()\n",
    "cluster_name_orig_list.sort(key=lambda x: len(x.split()), reverse=True)\n",
    "\n",
    "# create dict to map recipe_id to ingredient_ids\n",
    "df_ingredients = df_ingredients_raw.groupby('recipe_id')['ingredient_id'].apply(list).reset_index(name ='ingredients')\n",
    "dict_recipe_ingredients = dict(zip(df_ingredients.recipe_id, df_ingredients.ingredients))\n",
    "\n",
    "# create dict to map ingredient_id to cluster_name\n",
    "df_cluster = df_cluster.replace(' ', '_', regex=True)\n",
    "dict_ingredient_clustername = dict(zip(df_cluster.ingredient_id, df_cluster.cluster_name))\n",
    "\n",
    "# list of cluster names with underscore, e.g. apple_juice, so that when check 'apple', won't confuse with 'apple juice'\n",
    "cluster_name_list = df_cluster.cluster_name.tolist()\n",
    "cluster_name_list.sort(key=lambda x: len(x.split('_')), reverse=True)\n",
    "\n",
    "# create dict to map concatenated_cluster_names to original cluster names\n",
    "# e.g. 'apple_juice: apple juice'\n",
    "dict_ingre_concat_to_origin = dict(zip(cluster_name_list, [ingre.replace('_',' ') for ingre in cluster_name_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict to map each recipe id to its sorted ingredients' cluster names\n",
    "dict_recipe_to_name = {}\n",
    "for recipe_id in dict_recipe_ingredients.keys():\n",
    "    try:\n",
    "        recipe = [dict_ingredient_clustername[ingre_id] for ingre_id in dict_recipe_ingredients[recipe_id]]\n",
    "        recipe.sort(key=lambda x: len(x.split('_')), reverse=True)\n",
    "        dict_recipe_to_name[recipe_id] = recipe\n",
    "    except:\n",
    "        continue # if this recipe ID has no ingredient info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions - to be called in the main text processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for expanding contractions in case of negations\n",
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_nouns=['cup','teaspoon','fluid ounce','ounce',\n",
    "             'tablespoon','can or bottle','bottle',\n",
    "             'can','package','pound','inch','jar',\n",
    "             'packet','carton','bottle','milliliter',\n",
    "             'box','bunch','tube','container',\n",
    "             'jigger','quart','slice','liter','pouch',\n",
    "             'container','bag','pint','bar', 'gallon']\n",
    "amount_units = ['lb', 'oz']\n",
    "\n",
    "# lemmatization dict\n",
    "lemma_list = pd.read_csv('input_data/lemma_list.csv')\n",
    "lemma_dict = lemma_list.set_index('word_list').to_dict()['lemma_list']\n",
    "def lemmatization(text):\n",
    "    # list of words that are forced not to lemmatize, those are the words appearing in cluster names\n",
    "    force_keep_list = ['corned', 'sparkling', 'canning', 'roasted', 'baked', 'processed', 'flavored', \n",
    "                       'colored', 'candied', 'stuffing', 'dressing', 'shortening', \"pig's\", 'based',\n",
    "                       'stewed', 'curing', 'decorating', 'coated', 'evaporated', 'pickled', 'fried',\n",
    "                       'dripping', 'rising', \"confectioners'\", 'frying', 'coating', 'smoked', 'seasoned',\n",
    "                       'rolled', 'filling', \"devil's\", 'sweetened', 'dried', 'pickling', 'topping', 'frosting',\n",
    "                       'coloring', 'rose', 'pulled', 'crystallized', 'seasoning', 'whipped', 'condensed','baking',\n",
    "                      'frenchfries', 'fries', 'flavoring']\n",
    "    text = text.lower()\n",
    "    text = text.replace('-n-', ' and ')\n",
    "    text = text.replace(' & ', ' and ')\n",
    "    text = text.replace('&', ' and ')\n",
    "    text = text.replace('-', ' ')\n",
    "    text = text.replace('/', '')\n",
    "    text = text.replace('sugar substitute', 'sweetener').replace('french fries','frenchfries')\n",
    "    text = text.replace('dry milk', 'milk powder').replace('powder milk', 'milk powder')\n",
    "    text = text.replace('lowfat',\n",
    "                        'low fat').replace('nonfat',\n",
    "                                                    'non fat').replace('glutenfree',\n",
    "                                                                       'gluten free').replace('corn flakes',\n",
    "                                                                                              'cornflakes')\n",
    "    text = text.replace('flaxseed',\n",
    "                        'flax seed').replace('lemongrass', 'lemon grass')\n",
    "    text = text.replace('coconutmilk',\n",
    "                        'coconut milk').replace('almondmilk',\n",
    "                                                'almond milk').replace('crab meat',\n",
    "                                                                       'crabmeat').replace('starfruit', \n",
    "                                                                                           'star fruit').replace('breadcrumb', \n",
    "                                                                                                                 'bread crumb')\n",
    "    text = text.replace('red and yellow bell pepper', \n",
    "                        'red bell pepper and yellow bell pepper').replace('red and green bell pepper', \n",
    "                                                                          'red bell pepper and green bell pepper')\n",
    "\n",
    "    text = text.replace('used to', '')\n",
    "    word_list = word_tokenize(text)\n",
    "    word_list_after = []\n",
    "    \n",
    "    # use the custom lemma dict first\n",
    "    word_list = [str(lemma_dict.get(word, word)) for word in word_list]\n",
    "    \n",
    "    # then use the WordNetLemmatizer from nltk\n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for word in word_list:\n",
    "        # word by word (otherwise tag may differ), lemmatize each word based on its pos tagging, exclude words in force keep list\n",
    "        w,t = pos_tag([word])[0]\n",
    "        if t[0].lower() in ['a','n','v'] and word not in force_keep_list:\n",
    "            word = wnl.lemmatize(word,t[0].lower())\n",
    "        word_list_after.append(word)\n",
    "    \n",
    "    text = ' '.join(word_list_after)\n",
    "    \n",
    "    # remove the terms indicating amount, e.g. \"2 to 3 cup of beer\"\n",
    "    for amount_noun in amount_nouns:\n",
    "        text = re.sub(r'([0-9]* to )*[0-9]* ' + amount_noun + r' of', '', text)\n",
    "    # remove more terms indicating amount, e.g. \"use a 4 lb roast\"\n",
    "    for amount_unit in amount_units:\n",
    "        text = re.sub(r'([0-9]* to )*[0-9]*( )*' + amount_unit, '', text)\n",
    "    \n",
    "    # remove all numbers\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    \n",
    "    text = ' '.join(text.split()) # replace multiple spaces as single space\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "\n",
    "# print(lemmatization('I used 2 to 3 cups of beer together with the milk'))\n",
    "# print(lemmatization('I used 3 cups of beer together with the milk'))\n",
    "# print(lemmatization('I used cups of beer together with the milk'))\n",
    "# print(lemmatization('I used a 4 to 5 lb chuck roast and 2oz beer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_with_ingredient(text, recipe):\n",
    "    # tokenize the review text\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    tokens = list(itertools.chain.from_iterable(tokens_sentences))\n",
    "    flag = False\n",
    "    \n",
    "    # detect ingredients and get ngrams\n",
    "    ingre_in_recipe = []\n",
    "    ingre_not_recipe = []\n",
    "    detected_ingredients = [token.replace('INGRE_','') for token in tokens if 'INGRE_' in token]\n",
    "    if len(detected_ingredients) > 0:\n",
    "        flag = True\n",
    "    for ingre in detected_ingredients:\n",
    "        if ingre in recipe:\n",
    "            ingre_in_recipe.append(ingre)\n",
    "        else:\n",
    "            ingre_not_recipe.append(ingre)\n",
    "    \n",
    "    \n",
    "    return flag, list(set(ingre_in_recipe)), list(set(ingre_not_recipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_phrase(text):\n",
    "    # concatenate some important phrases for the ease of detection\n",
    "    text = re.sub(r'\\binstead of\\b', 'instead_of', text)\n",
    "    text = re.sub(r'\\brather than\\b', 'rather_than', text)\n",
    "    text = re.sub(r'\\bleave out\\b', 'leave_out', text)\n",
    "    text = re.sub(r'\\bleave off\\b', 'leave_off', text)\n",
    "    text = re.sub(r'\\bbother with\\b', 'bother_with', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_accented_characters(text):\n",
    "    \"\"\"\n",
    "    In case of Unicode characters\n",
    "    \"\"\"\n",
    "    # text = unicodedata.normalize('NFKD', text)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\").replace(\"‘\", \"'\").replace('–', '-')\n",
    "    text = re.sub('ı', 'I', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \"\"\"\n",
    "    Expand contractions in case of negations, e.g. isn’t -> is not\n",
    "    \"\"\"\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) \\\n",
    "            if contraction_mapping.get(match) \\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def replacement_extraction(text, recipe):\n",
    "    \"\"\"\n",
    "    extract Replacement terms\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    tagged_sentences = [tagger.tag(sent) for sent in tokens_sentences]\n",
    "    DESC_list = []\n",
    "    for sent_tagged in tagged_sentences:  # each sentence\n",
    "        sent_tree = key_phrases_cp.parse(sent_tagged)  # chunk each sentence based on grammar structure\n",
    "        DESC_in_this_sent = []\n",
    "        for subtree in sent_tree.subtrees():\n",
    "            if subtree.label() == \"REPLACEMENT_TERM\":\n",
    "                string = str(subtree)\n",
    "                string = re.sub('\\n', '', string)\n",
    "                string = re.sub('\\s+', ' ', string)\n",
    "                string = re.sub('REPLACEMENT_TERM ', '', string)\n",
    "                string = re.sub('\\/[A-Z]+\\$*', '', string)\n",
    "                string = re.sub('\\(', '', string)\n",
    "                string = re.sub('\\)', '', string)\n",
    "\n",
    "                DESC_in_this_sent.append(string)\n",
    "        if DESC_in_this_sent != []:\n",
    "            DESC_list.extend(DESC_in_this_sent)\n",
    "    DESC_list = list(set(DESC_list))\n",
    "    return DESC_list\n",
    "\n",
    "\n",
    "def addition_extraction(text, recipe):\n",
    "    \"\"\"\n",
    "    extract Addition terms\n",
    "    \"\"\"\n",
    "    prefix_recipe = [prefix+ingre for ingre in recipe]\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    tagged_sentences = [tagger.tag(sent) for sent in tokens_sentences]\n",
    "    false_additions = []\n",
    "    real_additions = []\n",
    "    for sent_tagged in tagged_sentences:  # each sentence\n",
    "        sent_tree = key_phrases_cp.parse(sent_tagged)  # chunk each sentence based on grammar structure\n",
    "        false_additions_in_this_sent = []\n",
    "        real_additions_in_this_sent = []\n",
    "        for subtree in sent_tree.subtrees():\n",
    "            if subtree.label() == \"ADDITION_TERM\":\n",
    "                word_list = []\n",
    "                # check whether the ingredient word in this term is in the original recipe\n",
    "                flag = True # flag is true if the ingredient word is in the recipe\n",
    "                for (word, tag) in subtree.leaves():\n",
    "                    word_list.append(word)\n",
    "                    if tag == 'INGREDIENT':\n",
    "                        if word not in prefix_recipe:\n",
    "                            flag = False\n",
    "                string = ' '.join(word_list)\n",
    "                if flag == True: # if the ingredient is in the recipe, it is a false addition (e.g. modify quantities)\n",
    "                    false_additions_in_this_sent.append(string)\n",
    "                else: # if the ingredient is not in the recipe, it is a real addition\n",
    "                    real_additions_in_this_sent.append(string)\n",
    "\n",
    "        if false_additions_in_this_sent != []:\n",
    "            false_additions.extend(false_additions_in_this_sent)\n",
    "        if real_additions_in_this_sent != []:\n",
    "            real_additions.extend(real_additions_in_this_sent)\n",
    "    false_additions = list(set(false_additions))\n",
    "    real_additions = list(set(real_additions))\n",
    "    return false_additions, real_additions\n",
    "\n",
    "\n",
    "def deletion_extraction(text, recipe):\n",
    "    \"\"\"\n",
    "    extract Deletion terms\n",
    "    \"\"\"\n",
    "    prefix_recipe = [prefix+ingre for ingre in recipe]\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    tagged_sentences = [tagger.tag(sent) for sent in tokens_sentences]\n",
    "    false_deletions = []\n",
    "    real_deletions = []\n",
    "    for sent_tagged in tagged_sentences:  # each sentence\n",
    "        sent_tree = key_phrases_cp.parse(sent_tagged)  # chunk each sentence based on grammar structure\n",
    "        false_deletions_in_this_sent = []\n",
    "        real_deletions_in_this_sent = []\n",
    "        for subtree in sent_tree.subtrees():\n",
    "            if subtree.label() == \"DELETION_TERM\":\n",
    "                word_list = []\n",
    "                # check whether the ingredient word in this term is in the original recipe\n",
    "                flag = True # flag is true if the ingredient word is in the recipe\n",
    "                for (word, tag) in subtree.leaves():\n",
    "                    word_list.append(word)\n",
    "                    if tag == 'INGREDIENT':\n",
    "                        if word not in prefix_recipe:\n",
    "                            flag = False\n",
    "                string = ' '.join(word_list)\n",
    "                if flag == False: # if the ingredient is not in the recipe, it is a false deletion\n",
    "                                  # e.g. \"It's good that the recipe contains no milk cuz I don't have any milk\"\n",
    "                    false_deletions_in_this_sent.append(string)\n",
    "                else: # if the ingredient is in the recipe, it is a real deletion\n",
    "                    real_deletions_in_this_sent.append(string)\n",
    "\n",
    "        if false_deletions_in_this_sent != []:\n",
    "            false_deletions.extend(false_deletions_in_this_sent)\n",
    "        if real_deletions_in_this_sent != []:\n",
    "            real_deletions.extend(real_deletions_in_this_sent)\n",
    "    false_deletions = list(set(false_deletions))\n",
    "    real_deletions = list(set(real_deletions))\n",
    "    return false_deletions, real_deletions\n",
    "\n",
    "\n",
    "\n",
    "# Custom our own tagger for POS tagging, and use python's default tagger as backup\n",
    "custom_tagger = {\n",
    "    # replacement\n",
    "    'replace': 'REPLACEMENT', 'substitute': 'REPLACEMENT', 'instead_of': 'REPLACEMENT', 'rather_than': 'REPLACEMENT',\n",
    "    # deletion\n",
    "    'delete': 'DELETION', 'remove': 'DELETION', 'omit': 'DELETION', 'subtract': 'DELETION', 'skip': 'DELETION',\n",
    "    'eliminate': 'DELETION', \n",
    "    # 'discard': 'DELETION', \n",
    "    'leave_out': 'DELETION', 'leave_off': 'DELETION',\n",
    "    'bother_with': 'BOTHERWITH',\n",
    "    # addition\n",
    "    'add': 'ADDITION', 'use': 'ADDITION', 'put': 'ADDITION', 'incorporate': 'ADDITION',\n",
    "    # other tags\n",
    "    'no': 'NO',\n",
    "    'not': 'NOT',\n",
    "    'that': 'DT',\n",
    "    'with': 'IN',\n",
    "    'even': 'EVEN',\n",
    "    'have': 'HAVE',\n",
    "    'like': 'LIKE',\n",
    "    # punctuations\n",
    "    ',': 'PUNC'\n",
    "}\n",
    "\n",
    "ingredient_tagger = {}\n",
    "for ingre in cluster_name_list:\n",
    "    ingredient_tagger[prefix+ingre] = 'INGREDIENT'\n",
    "    \n",
    "custom_tagger_combined = {**custom_tagger, **ingredient_tagger}\n",
    "\n",
    "default_tagger = nltk.data.load(path_to_nltk_tagger)\n",
    "tagger = nltk.tag.UnigramTagger(model=custom_tagger_combined, backoff=default_tagger)\n",
    "\n",
    "# Define grammar structures for extracting negation phrases and then creativity words\n",
    "key_phrases_grammar = r\"\"\"\n",
    "    REPLACEMENT_TERM:\n",
    "        # e.g. don't have/like apple, so ADDITION(add/use/put/...) banana\n",
    "        {<NOT>(<HAVE>|<LIKE>)<[A-Z]+>{0,4}<INGREDIENT><[A-Z]+>{0,4}<ADDITION><[A-Z]+>{0,4}<INGREDIENT>}\n",
    "        \n",
    "        # e.g. have no apple so ADDITION(add/use/put/...) banana\n",
    "        {<HAVE><NO><INGREDIENT><[A-Z]+>{0,4}<ADDITION><[A-Z]+>{0,4}<INGREDIENT>}\n",
    "        \n",
    "        # e.g. ADDITION(add/use/put/...) banana cuz don't have/like apple\n",
    "        {<ADDITION><[A-Z]+>{0,4}<INGREDIENT><[A-Z]+>{0,4}<NOT>(<HAVE>|<LIKE>)<[A-Z]+>{0,4}<INGREDIENT>}\n",
    "        \n",
    "        # e.g. ADDITION(add/use/put/...) banana cuz have no apple\n",
    "        {<ADDITION><[A-Z]*>{0,4}<INGREDIENT><[A-Z]*>{0,4}<HAVE><NO><INGREDIENT>}\n",
    "        \n",
    "        # e.g. ADDITION(add/use/put/...) banana instead of apple\n",
    "        {<ADDITION><[A-Z]*>{0,4}<INGREDIENT><REPLACEMENT><INGREDIENT>}\n",
    "        \n",
    "        # e.g. instead of apple I ADDITION(add/use/put/...) banana \n",
    "        {<REPLACEMENT><INGREDIENT><[A-Z]*>{0,4}<ADDITION><[A-Z]*>{0,4}<INGREDIENT>}\n",
    "        \n",
    "        # e.g. replace the Apple with some Banana\n",
    "        {<REPLACEMENT><[A-Z]*>{0,4}<INGREDIENT><IN><DT>?<[A-Z]*>{0,4}<INGREDIENT>}\n",
    "        \n",
    "        # e.g. didn't use Apple, replace with Banana\n",
    "        {<INGREDIENT><[A-Z]*>{0,4}<REPLACEMENT><IN><[A-Z]*>{0,4}<INGREDIENT>}\n",
    "        \n",
    "        # <DELETION> ingre and <REPLACEMENT> ingre\n",
    "        # e.g. I omitted apple and substitute with banana\n",
    "        {<DELETION><[A-Z]*>{0,4}<INGREDIENT><[A-Z]*>{0,4}<REPLACEMENT><[A-Z]*>*<INGREDIENT>}\n",
    "        \n",
    "    DELETION_TERM:\n",
    "        ### Extract negation with \"have\" \n",
    "        # not have + ingredient\"\n",
    "        # e.g. \"I didn't have any Beef\"\n",
    "        {<NOT><HAVE><[A-Z]*>{0,4}<INGREDIENT>}\n",
    "        \n",
    "        # \"have no + ingredient\"\n",
    "        # e.g. \"I had no Beef\"\n",
    "        {<HAVE><NO><INGREDIENT>}\n",
    "        \n",
    "        ### Extract negation with addition words\n",
    "        # e.g. \"I didn't add any water\", \"I use no milk\"\n",
    "        {<NOT><ADDITION><[A-Z]*>{0,4}<INGREDIENT>}\n",
    "        {<ADDITION><NO><INGREDIENT>}\n",
    "        \n",
    "        ### Extract \"deletion + ingredient\"\n",
    "        # e.g. \"I removed Apple\", \"I deleted the Beef\", \"I left off the Beef\"\n",
    "        {<DELETION><[A-Z]*>{0,3}<INGREDIENT>}\n",
    "        \n",
    "        ### Extract negation terms with \"bother with\"\n",
    "        # e.g. \"I didn't bother with the Beef\"\n",
    "        {<NOT><BOTHERWITH><[A-Z]*>*<INGREDIENT>}      \n",
    "    \n",
    "    ADDITION_TERM:\n",
    "        # e.g. I added {0-4 words such as \"some\", \"a bit\"} Milk\n",
    "        {<ADDITION><[A-Z]*>{0,4}<INGREDIENT>}\n",
    "\"\"\"\n",
    "key_phrases_cp = nltk.RegexpParser(key_phrases_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_ingredient_in_review(text, recipe, seq=3):\n",
    "    '''\n",
    "    Standardize all mentionings of ingre in the review to its cluster name\n",
    "    (including short/parent/child/synonym name matching in mapping dict)\n",
    "\n",
    "    Example:\n",
    "    Input text: I omit chocolate chip.\n",
    "    Recipe: [white_chocolate_chip, others...], while both \"white chocolate chip\" & \"chocolate chip\" are cluster names\n",
    "    Output text: I omit white_chocolate_chip.\n",
    "    \n",
    "    Also output the mappings that happened in the text.\n",
    "    \n",
    "    Sub-functions that can be freely combined in different sequences (some default seq (i.e. seq 1,2,3) has been set):\n",
    "        1. full_recipe_ingre_match\n",
    "        2. full_nonrecipe_ingre_match\n",
    "        merged 1,2: full_ingre_match\n",
    "        3. partial_recipe_ingre_match\n",
    "        4. synonym_recipe_ingre_match\n",
    "        5. parent_recipe_ingre_match\n",
    "        6. child_recipe_ingre_match\n",
    "        7. synonym_nonrecipe_ingre_match\n",
    "        8. child_nonrecipe_ingre_match\n",
    "        merged 4,5,6: synonym_child_parent_recipe_ingre_match\n",
    "        merged 3,4,5,6: partial_synonym_child_parent_recipe_ingre_match\n",
    "        merged 7,8: synonym_child_nonrecipe_ingre_match\n",
    "    '''\n",
    "    \n",
    "    # list of full-match ingre in this text\n",
    "    full_match_in_this_text = []\n",
    "    # list of pairs of other matches (partial, synonym, parent, child names) happen in this text\n",
    "    partial_match_in_this_text = []\n",
    "    synonym_match_in_this_text = []\n",
    "    parent_match_in_this_text = []\n",
    "    child_match_in_this_text = []\n",
    "    # list of detected new spice\n",
    "    new_spice_ls = []\n",
    "    \n",
    "    # keep (and sort) a list of nonrecipe-ingre that are not matched in the review text yet\n",
    "    non_recipe_ingre_not_matched = list(set(cluster_name_list) - set(recipe))\n",
    "    non_recipe_ingre_not_matched.sort(key=lambda x: len(x.split('_')), reverse=True)\n",
    "    \n",
    "    def detect_new_spice(text):\n",
    "        name = 'spice'\n",
    "        tokens = text.split()\n",
    "        word_before_spice = tokens[tokens.index(name) - 1]\n",
    "        if prefix in word_before_spice: # if the word before \"spice\" is an INGRE\n",
    "            new_spice = word_before_spice + '_' + name\n",
    "            text = re.sub(r'\\b' + word_before_spice +' ' + name + r'\\b', 'INGRE_mixed_spice', text)\n",
    "            new_spice_ls.append(new_spice)\n",
    "        return text\n",
    "    \n",
    "    def full_recipe_ingre_match(text = text):\n",
    "        # concatenate the full-match recipe-ingre with '_' in review\n",
    "        # since the recipe is sorted, longest ingredient will be matched first\n",
    "        # e.g. \"white chocolate chip\" will be detected and replaced by \"white_chocolate_chip\", not \"chocolate_chip\"\n",
    "        for ingre in recipe:\n",
    "            ingre_origin = dict_ingre_concat_to_origin[ingre]\n",
    "            if re.search(r'\\b' + ingre_origin + r'\\b', text) is not None: # a (whole-word) full-match exist\n",
    "                text = re.sub(r'\\b' + ingre_origin + r'\\b', prefix+ingre, text) # replace with concatenated version\n",
    "                full_match_in_this_text.append(ingre)\n",
    "        return text\n",
    "    \n",
    "    def full_nonrecipe_ingre_match(text = text, non_recipe_ingre_not_matched = non_recipe_ingre_not_matched):\n",
    "        # concatenate the full-match nonrecipe-ingre in the review\n",
    "        for ingre in non_recipe_ingre_not_matched:\n",
    "            ingre_origin = dict_ingre_concat_to_origin[ingre]\n",
    "            if re.search(r'\\b' + ingre_origin + r'\\b', text) is not None: # a (whole-word) full-match exist\n",
    "                text = re.sub(r'\\b' + ingre_origin + r'\\b', prefix+ingre, text) # replace with concatenated version\n",
    "                non_recipe_ingre_not_matched.remove(ingre)\n",
    "                full_match_in_this_text.append(ingre)    \n",
    "        return text\n",
    "    \n",
    "    def full_ingre_match(text, non_recipe_ingre_not_matched = non_recipe_ingre_not_matched):\n",
    "        # concatenate the full-match ingre (both recipe and non-recipe ingre) in the review from the longest\n",
    "        for ingre in cluster_name_list:\n",
    "            ingre_origin = dict_ingre_concat_to_origin[ingre]\n",
    "            if re.search(r'\\b' + ingre_origin + r'\\b', text) is not None: # a (whole-word) full-match exist\n",
    "                text = re.sub(r'\\b' + ingre_origin + r'\\b', prefix+ingre, text) # replace with concatenated version\n",
    "                if ingre in non_recipe_ingre_not_matched:\n",
    "                    non_recipe_ingre_not_matched.remove(ingre)\n",
    "                full_match_in_this_text.append(ingre)    \n",
    "        return text\n",
    "            \n",
    "    \n",
    "    def partial_recipe_ingre_match(text = text):\n",
    "        # for recipe-ingredients only, check whether short names exists in review,\n",
    "        # if yes then replace it with cluster name\n",
    "        for ingre in recipe:\n",
    "            if ingre in dict_ingre_mapping.keys(): # if ingre has other names in dict\n",
    "                short = dict_ingre_mapping[ingre]['short']\n",
    "                all_possible_names = short\n",
    "                # remove the names that are overlaped with recipe-ingre\n",
    "                # e.g. \"chocolate\" is short for \"chocolate chip\", but \"chocolate\" could also be a recipe-ingre,\n",
    "                #       in this case, won't replace \"chocolate\" in review with \"chocolate chip\"\n",
    "                all_possible_names = list(set(all_possible_names) - set(recipe)) \n",
    "                all_possible_names.sort(key=lambda x: len(x.split('_')), reverse=True) # sort all names with length\n",
    "                for name in all_possible_names:\n",
    "                    name_origin = name.replace('_',' ')\n",
    "                    if re.search(r'\\b' + name_origin + r'\\b', text) is not None: # if this name (whole-word) found in text\n",
    "                        if name == 'spice':\n",
    "                            text = detect_new_spice(text)\n",
    "                        else:\n",
    "                            # replace this name with concatenated cluster name\n",
    "                            text = re.sub(r'\\b' + name_origin + r'\\b', prefix+ingre, text) \n",
    "                            partial_match_in_this_text.append([name, ingre])\n",
    "        return text\n",
    "                        \n",
    "    def parent_recipe_ingre_match(text = text):\n",
    "        # for recipe-ingredients only, check whether parent names exists in review,\n",
    "        # if yes then replace this parent name with its cluster name\n",
    "        for ingre in recipe:\n",
    "            if ingre in dict_ingre_mapping.keys(): # if ingre has other names in dict\n",
    "                parent = dict_ingre_mapping[ingre]['parent']\n",
    "                all_possible_names = parent\n",
    "                # remove the names that are overlaped with recipe-ingre\n",
    "                all_possible_names = list(set(all_possible_names) - set(recipe)) \n",
    "                all_possible_names.sort(key=lambda x: len(x.split('_')), reverse=True) # sort all names with length\n",
    "                for name in all_possible_names:\n",
    "                    name_origin = name.replace('_',' ')\n",
    "                    if re.search(r'\\b' + name_origin + r'\\b', text) is not None: # if this name (whole-word) found in text\n",
    "                        if name == 'spice':\n",
    "                            text = detect_new_spice(text)\n",
    "                        else:\n",
    "                            # replace this name with concatenated cluster name\n",
    "                            text = re.sub(r'\\b' + name_origin + r'\\b', prefix+ingre, text) \n",
    "                            parent_match_in_this_text.append([name, ingre])\n",
    "        return text\n",
    "    \n",
    "    def synonym_recipe_ingre_match(text = text):\n",
    "        # for recipe-ingredients only, check whether synonym names exists in review,\n",
    "        # if yes then replace it with cluster name\n",
    "        for ingre in recipe:\n",
    "            if ingre in dict_ingre_mapping.keys(): # if ingre has other names in dict\n",
    "                synonym = dict_ingre_mapping[ingre]['synonym']\n",
    "                all_possible_names = synonym\n",
    "                # remove the names that are overlaped with recipe-ingre\n",
    "                all_possible_names = list(set(all_possible_names) - set(recipe)) \n",
    "                all_possible_names.sort(key=lambda x: len(x.split('_')), reverse=True) # sort all names with length\n",
    "                for name in all_possible_names:\n",
    "                    name_origin = name.replace('_',' ')\n",
    "                    if re.search(r'\\b' + name_origin + r'\\b', text) is not None: # if this name (whole-word) found in text\n",
    "                        if name == 'spice':\n",
    "                            text = detect_new_spice(text)\n",
    "                        else:\n",
    "                            # replace this name with concatenated cluster name\n",
    "                            text = re.sub(r'\\b' + name_origin + r'\\b', prefix+ingre, text) \n",
    "                            synonym_match_in_this_text.append([name, ingre])\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def child_recipe_ingre_match(text = text):\n",
    "        # for recipe-ingredients only, check whether child names exists in review,\n",
    "        # if yes then replace this child name with its cluster name\n",
    "        for ingre in recipe:\n",
    "            if ingre in dict_ingre_mapping.keys(): # if ingre has other names in dict\n",
    "                child = dict_ingre_mapping[ingre]['child']\n",
    "                all_possible_names = child\n",
    "                # remove the names that are overlaped with recipe-ingre\n",
    "                all_possible_names = list(set(all_possible_names) - set(recipe)) \n",
    "                all_possible_names.sort(key=lambda x: len(x.split('_')), reverse=True) # sort all names with length\n",
    "                for name in all_possible_names:\n",
    "                    name_origin = name.replace('_',' ')\n",
    "                    if re.search(r'\\b' + name_origin + r'\\b', text) is not None: # if this name (whole-word) found in text\n",
    "                        if name == 'spice':\n",
    "                            text = detect_new_spice(text)\n",
    "                        else:\n",
    "                            # replace this name with concatenated cluster name\n",
    "                            text = re.sub(r'\\b' + name_origin + r'\\b', prefix+ingre, text) \n",
    "                            child_match_in_this_text.append([name, ingre])\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def child_nonrecipe_ingre_match(text = text, non_recipe_ingre_not_matched = non_recipe_ingre_not_matched):\n",
    "    # for remained unmatched nonrecipe-ingre\n",
    "    # check whether its child names are in the review and replace with its cluster name\n",
    "        for ingre in non_recipe_ingre_not_matched:\n",
    "            if ingre in dict_ingre_mapping.keys(): # if ingre has other names in dict\n",
    "                child = dict_ingre_mapping[ingre]['child']\n",
    "                all_possible_names = child\n",
    "                all_possible_names.sort(key=lambda x: len(x.split('_')), reverse=True) # sort all names with length\n",
    "                for name in all_possible_names:\n",
    "                    if name not in recipe: # in case the name is a recipe-ingre\n",
    "                        name_origin = name.replace('_',' ')\n",
    "                        if re.search(r'\\b' + name_origin + r'\\b', text) is not None: # if this name (whole-word) found in text\n",
    "                            if name == 'spice':\n",
    "                                text = detect_new_spice(text)\n",
    "                            else:\n",
    "                                # replace its names in dict with full-length cluster name\n",
    "                                text = re.sub(r'\\b' + name_origin + r'\\b', prefix+ingre, text)\n",
    "                                child_match_in_this_text.append([name, ingre])\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def synonym_nonrecipe_ingre_match(text = text, non_recipe_ingre_not_matched = non_recipe_ingre_not_matched):\n",
    "    # for remained unmatched nonrecipe-ingre\n",
    "    # check whether its child names are in the review and replace with its cluster name\n",
    "        for ingre in non_recipe_ingre_not_matched:\n",
    "            if ingre in dict_ingre_mapping.keys(): # if ingre has other names in dict\n",
    "                synonym = dict_ingre_mapping[ingre]['synonym']\n",
    "                all_possible_names = synonym\n",
    "                all_possible_names.sort(key=lambda x: len(x.split('_')), reverse=True) # sort all names with length\n",
    "                for name in all_possible_names:\n",
    "                    if name not in recipe: # in case the name is a recipe-ingre\n",
    "                        name_origin = name.replace('_',' ')\n",
    "                        if re.search(r'\\b' + name_origin + r'\\b', text) is not None: # if this name (whole-word) found in text\n",
    "                            if name == 'spice':\n",
    "                                text = detect_new_spice(text)\n",
    "                            else:\n",
    "                                # replace its names in dict with full-length cluster name\n",
    "                                text = re.sub(r'\\b' + name_origin + r'\\b', prefix+ingre, text)\n",
    "                                synonym_match_in_this_text.append([name, ingre])\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def synonym_child_nonrecipe_ingre_match(text = text, non_recipe_ingre_not_matched = non_recipe_ingre_not_matched):\n",
    "    # for remained unmatched nonrecipe-ingre\n",
    "    # check whether its merged synonym & child names are in the review and replace with its cluster name\n",
    "        for ingre in non_recipe_ingre_not_matched:\n",
    "            if ingre in dict_ingre_mapping.keys(): # if ingre has other names in dict\n",
    "                synonym = dict_ingre_mapping[ingre]['synonym']\n",
    "                child = dict_ingre_mapping[ingre]['child']\n",
    "                all_possible_names = synonym + child\n",
    "                all_possible_names.sort(key=lambda x: len(x.split('_')), reverse=True) # sort all names with length\n",
    "                for name in all_possible_names:\n",
    "                    if name not in recipe: # in case the name is a recipe-ingre\n",
    "                        name_origin = name.replace('_',' ')\n",
    "                        if re.search(r'\\b' + name_origin + r'\\b', text) is not None: # if this name (whole-word) found in text\n",
    "                            if name == 'spice':\n",
    "                                text = detect_new_spice(text)\n",
    "                            else:\n",
    "                                # replace its names in dict with full-length cluster name\n",
    "                                text = re.sub(r'\\b' + name_origin + r'\\b', prefix+ingre, text)\n",
    "                                if name in synonym:\n",
    "                                    synonym_match_in_this_text.append([name, ingre])\n",
    "                                else:\n",
    "                                    child_match_in_this_text.append([name, ingre])\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def synonym_child_parent_recipe_ingre_match(text = text):\n",
    "    # for recipe-ingre\n",
    "    # check whether its merged synonym & child & parent names are in the review and replace with its cluster name\n",
    "        for ingre in recipe:\n",
    "            if ingre in dict_ingre_mapping.keys(): # if ingre has other names in dict\n",
    "                child = dict_ingre_mapping[ingre]['child']\n",
    "                synonym = dict_ingre_mapping[ingre]['synonym']\n",
    "                parent = dict_ingre_mapping[ingre]['parent']\n",
    "                all_possible_names = child + synonym + parent\n",
    "                # remove the names that are overlaped with recipe-ingre\n",
    "                all_possible_names = list(set(all_possible_names) - set(recipe)) \n",
    "                all_possible_names.sort(key=lambda x: len(x.split('_')), reverse=True) # sort all names with length\n",
    "                for name in all_possible_names:\n",
    "                    name_origin = name.replace('_',' ')\n",
    "                    if re.search(r'\\b' + name_origin + r'\\b', text) is not None: # if this name (whole-word) found in text\n",
    "                        if name == 'spice':\n",
    "                            text = detect_new_spice(text)\n",
    "                        else:\n",
    "                            # replace this name with concatenated cluster name\n",
    "                            text = re.sub(r'\\b' + name_origin + r'\\b', prefix+ingre, text) \n",
    "                            child_match_in_this_text.append([name, ingre])\n",
    "                            if name in synonym:\n",
    "                                synonym_match_in_this_text.append([name, ingre])\n",
    "                            elif name in child:\n",
    "                                child_match_in_this_text.append([name, ingre])\n",
    "                            else:\n",
    "                                parent_match_in_this_text.append([name, ingre])\n",
    "        return text\n",
    "    \n",
    "    def partial_synonym_child_parent_recipe_ingre_match(text = text):\n",
    "    # for recipe-ingre\n",
    "    # check whether its merged synonym & child & parent names are in the review and replace with its cluster name\n",
    "        for ingre in recipe:\n",
    "            if ingre in dict_ingre_mapping.keys(): # if ingre has other names in dict\n",
    "                child = dict_ingre_mapping[ingre]['child']\n",
    "                synonym = dict_ingre_mapping[ingre]['synonym']\n",
    "                parent = dict_ingre_mapping[ingre]['parent']\n",
    "                all_possible_names = child + synonym + parent\n",
    "                # remove the names that are overlaped with recipe-ingre\n",
    "                all_possible_names = list(set(all_possible_names) - set(recipe)) \n",
    "                all_possible_names.sort(key=lambda x: len(x.split('_')), reverse=True) # sort all names with length\n",
    "                for name in all_possible_names:\n",
    "                    name_origin = name.replace('_',' ')\n",
    "                    if re.search(r'\\b' + name_origin + r'\\b', text) is not None: # if this name (whole-word) found in text\n",
    "                        if name == 'spice':\n",
    "                            text = detect_new_spice(text)\n",
    "                        else:\n",
    "                            # replace this name with concatenated cluster name\n",
    "                            text = re.sub(r'\\b' + name_origin + r'\\b', prefix+ingre, text) \n",
    "                            child_match_in_this_text.append([name, ingre])\n",
    "                            if name in synonym:\n",
    "                                synonym_match_in_this_text.append([name, ingre])\n",
    "                            elif name in child:\n",
    "                                child_match_in_this_text.append([name, ingre])\n",
    "                            elif name in parent:\n",
    "                                parent_match_in_this_text.append([name, ingre])\n",
    "                            else:\n",
    "                                partial_match_in_this_text.append([name, ingre])\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    if seq == 1:\n",
    "        ## sequence 1\n",
    "        text = full_ingre_match(text, non_recipe_ingre_not_matched) # merge 1&2, match from longest\n",
    "        text = partial_recipe_ingre_match(text) # 3\n",
    "        text = synonym_recipe_ingre_match(text) # 4\n",
    "        text = parent_recipe_ingre_match(text) # 5\n",
    "        text = child_recipe_ingre_match(text) # 6\n",
    "        text = synonym_nonrecipe_ingre_match(text, non_recipe_ingre_not_matched) # 7\n",
    "        text = child_nonrecipe_ingre_match(text, non_recipe_ingre_not_matched) # 8\n",
    "    \n",
    "    elif seq == 2:\n",
    "        ## sequence 2\n",
    "        text = full_ingre_match(text, non_recipe_ingre_not_matched) # merge 1&2, match from longest\n",
    "        text = partial_recipe_ingre_match(text) # 3\n",
    "        text = synonym_child_parent_recipe_ingre_match(text) # merge 4,5,6, match from longest\n",
    "        text = synonym_child_nonrecipe_ingre_match(text, non_recipe_ingre_not_matched) # merge 7,8, match from longest\n",
    "    \n",
    "    else:\n",
    "        ## sequence 3\n",
    "        text = full_ingre_match(text, non_recipe_ingre_not_matched) # merge 1&2, match from longest\n",
    "        text = partial_synonym_child_parent_recipe_ingre_match(text) # merge 3,4,5,6, match from longest\n",
    "        text = synonym_child_nonrecipe_ingre_match(text, non_recipe_ingre_not_matched) # merge 7,8, match from longest\n",
    "    \n",
    "    ### Note: The sub-functions 1&2 are not in-use in all 3 seq, below are how to use them separately\n",
    "    # text = full_recipe_ingre_match(text) # i.e. sub-function 1\n",
    "    # text = full_nonrecipe_ingre_match(text, non_recipe_ingre_not_matched) # i.e. sub-function 2\n",
    "        \n",
    "    return text, full_match_in_this_text, partial_match_in_this_text, \\\n",
    "           synonym_match_in_this_text, parent_match_in_this_text, child_match_in_this_text, new_spice_ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main text processing function\n",
    "\n",
    "### The output is the result df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(df, seq):\n",
    "    \"\"\"\n",
    "    text processing\n",
    "    \"\"\"\n",
    "    \n",
    "    recipe_id_list = df.recipe_id.tolist()\n",
    "    review_id_list = df.review_id.tolist()\n",
    "    review_list = df.review_text.tolist()\n",
    "    \n",
    "    contain_ingre_flag_list = []\n",
    "    addition_flag_list = []\n",
    "    deletion_flag_list = []\n",
    "    replacement_flag_list = []\n",
    "\n",
    "    ingre_in_recipe_list = []\n",
    "    ingre_not_recipe_list = []\n",
    "    ngram_in_list = []\n",
    "    ngram_not_list = []\n",
    "    recipe_list = []\n",
    "\n",
    "    real_addition_list = [] # terms caught by addition grammar but the ingredient is already in the recipe\n",
    "    false_addition_list = [] # terms caught by addition grammar and the ingredient is not in the recipe\n",
    "    real_deletion_list = [] # terms caught by deletion grammar and the ingredient is in the recipe\n",
    "    false_deletion_list = [] # terms caught by deletion grammar but the ingredient is not in the recipe\n",
    "    replacement_list = [] # terms indicating replacement\n",
    "\n",
    "    new_ingre_not_matched_list = [] # the ingredients not in recipe, but also isn't detected as an addition or replacement\n",
    "\n",
    "    clean_text_list = [] # review text after cleaning\n",
    "\n",
    "    full_match_list = [] # the full-match ingre in the review\n",
    "    partial_match_list = [] # the partial-match pairs ingre happened in the review, e.g. [\"juice\", \"apple_juice\"]\n",
    "    parent_match_list = [] # the parent-match pairs ingre happened in the review\n",
    "    child_match_list = [] # the child-match pairs ingre happened in the review\n",
    "    synonym_match_list = [] # the synonym-match pairs ingre happened in the review\n",
    "    new_spice_list = [] # the new spice detected in the review, will be replaced by \"mixed_spice\" in the text\n",
    "    \n",
    "    for i in trange(0,len(df)):\n",
    "        \n",
    "        # get the sorted recipe-ingredients for the review\n",
    "        recipe_id = df.iloc[i]['recipe_id']\n",
    "        try:\n",
    "            recipe = dict_recipe_to_name[recipe_id]\n",
    "        except:\n",
    "            recipe = [] # if the recipe id is not in the dict_recipe_to_name map list\n",
    "        \n",
    "        text = df.iloc[i]['review_text']\n",
    "        \n",
    "        # Convert to lower case\n",
    "        text = text.lower()\n",
    "\n",
    "        # Normalize the accented characters\n",
    "        text = normalize_accented_characters(text)\n",
    "\n",
    "        # Expand contractions, e.g. didn't --> did not\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        \n",
    "        # Lemmatize the text, e.g. removed --> remove\n",
    "        text = lemmatization(text)\n",
    "        \n",
    "        # concatenate some special phrases such as \"instead_of\"\n",
    "        text = concat_phrase(text)\n",
    "\n",
    "        # Standardize ingredient cluster names in the reviews\n",
    "        text, full_match, partial_match, synonym_match, parent_match, child_match, new_spice = standardize_ingredient_in_review(text, recipe, seq)\n",
    "        \n",
    "        clean_text_list.append(text)\n",
    "        full_match_list.append(full_match) # append the full-match that happened in this review\n",
    "        partial_match_list.append(partial_match) # append the other-match pairs that happened in this review\n",
    "        synonym_match_list.append(synonym_match) # append the other-match pairs that happened in this review\n",
    "        parent_match_list.append(parent_match) # append the other-match pairs that happened in this review\n",
    "        child_match_list.append(child_match) # append the other-match pairs that happened in this review\n",
    "        new_spice_list.append(new_spice) # append the new spices detected in this review\n",
    "        \n",
    "        # check whether review contain ingredient words in cluster name list, \n",
    "        # get the ingredients in the recipe and the ingredients not in the recipe\n",
    "        # also get the ngrams containing the detected ingredient words\n",
    "        flag, ingre_in_recipe, ingre_not_recipe = deal_with_ingredient(text, recipe)\n",
    "        contain_ingre_flag_list.append(flag)\n",
    "        ingre_in_recipe_list.append(ingre_in_recipe)\n",
    "        ingre_not_recipe_list.append(ingre_not_recipe)\n",
    "        \n",
    "        recipe_list.append(recipe)\n",
    "        \n",
    "        # Extract terms indicating altering\n",
    "        false_addition_terms, real_addition_terms = addition_extraction(text, recipe)\n",
    "        false_deletion_terms, real_deletion_terms = deletion_extraction(text, recipe)\n",
    "        replacement_terms = replacement_extraction(text, recipe)\n",
    "        \n",
    "        if len(real_addition_terms) > 0:\n",
    "            addition_flag_list.append(True)\n",
    "        else:\n",
    "            addition_flag_list.append(False)\n",
    "\n",
    "        if len(real_deletion_terms) > 0:\n",
    "            deletion_flag_list.append(True)\n",
    "        else:\n",
    "            deletion_flag_list.append(False)\n",
    "            \n",
    "        if len(replacement_terms) > 0:\n",
    "            replacement_flag_list.append(True)\n",
    "        else:\n",
    "            replacement_flag_list.append(False)\n",
    "            \n",
    "        # Filter the ingredients that are not in the recipe & not detected as any altering\n",
    "        new_ingre_not_matched = []\n",
    "        all_detected_terms = false_addition_terms + real_addition_terms + false_deletion_terms + real_deletion_terms + replacement_terms\n",
    "        all_detected_terms_tokens = ' '.join(all_detected_terms)\n",
    "        new_ingre_not_matched = [ingre for ingre in ingre_not_recipe if (prefix+ingre) not in all_detected_terms_tokens.split()]\n",
    "        \n",
    "        # append all the above results\n",
    "        replacement_list.append(replacement_terms)\n",
    "        real_addition_list.append(real_addition_terms)\n",
    "        false_addition_list.append(false_addition_terms)\n",
    "        real_deletion_list.append(real_deletion_terms)\n",
    "        false_deletion_list.append(false_deletion_terms)\n",
    "        new_ingre_not_matched_list.append(new_ingre_not_matched)\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'recipe_id': recipe_id_list,\n",
    "        'review_id': review_id_list,\n",
    "        'review_text': review_list,\n",
    "        'clean_text': clean_text_list,\n",
    "        'recipe': recipe_list,\n",
    "        'ingre_flag': contain_ingre_flag_list,\n",
    "        'ingre_in_recipe': ingre_in_recipe_list,\n",
    "        'ingre_not_recipe': ingre_not_recipe_list,\n",
    "        'addition_flag': addition_flag_list,\n",
    "        'deletion_flag': deletion_flag_list,\n",
    "        'replacement_flag': replacement_flag_list,\n",
    "        'real_addition_terms': real_addition_list,\n",
    "        'false_addition_terms': false_addition_list,\n",
    "        'real_deletion_terms': real_deletion_list,\n",
    "        'false_deletion_terms': false_deletion_list,\n",
    "        'replacement_terms': replacement_list,\n",
    "        'new_ingre_not_matched': new_ingre_not_matched_list,\n",
    "        'full_match': full_match_list,\n",
    "        'partial_match': partial_match_list,\n",
    "        'synonym_match': synonym_match_list,\n",
    "        'parent_match': parent_match_list,\n",
    "        'child_match': child_match_list,\n",
    "        'new_spice': new_spice_list\n",
    "    })\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the results for different sequence settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02df8998e3d45bc91e2c760ff21bd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cf6648ef304ec19be6b15e161fcab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ba68476d9749648ff6a147e297b456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: specify the sequence setting in text_processing function, as below\n",
    "\n",
    "result_df_1 = text_processing(df = df.head(10000), seq = 1)\n",
    "result_df_1.to_csv('OUTPUT_OF_FINAL_CODE/result_altering_seq1.csv')\n",
    "\n",
    "result_df_2 = text_processing(df = df.head(10000), seq = 2)\n",
    "result_df_2.to_csv('OUTPUT_OF_FINAL_CODE/result_altering_seq2.csv')\n",
    "\n",
    "result_df_3 = text_processing(df = df.head(10000), seq = 3)\n",
    "result_df_3.to_csv('OUTPUT_OF_FINAL_CODE/result_altering_seq3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stats df for seq 1,2,3 result, and also output the combined stats df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the result files\n",
    "result_seq1 = pd.read_csv('OUTPUT_OF_FINAL_CODE/review_altering_seq1.csv')\n",
    "result_seq2 = pd.read_csv('OUTPUT_OF_FINAL_CODE/review_altering_seq2.csv')\n",
    "result_seq3 = pd.read_csv('OUTPUT_OF_FINAL_CODE/review_altering_seq3.csv')\n",
    "\n",
    "# result for seq1:\n",
    "df = result_seq1\n",
    "\n",
    "num_ingre_in_recipe = [len(literal_eval(ls)) for ls in df.ingre_in_recipe]\n",
    "num_ingre_not_recipe = [len(literal_eval(ls)) for ls in df.ingre_not_recipe]\n",
    "num_real_addition_terms = [len(literal_eval(ls)) for ls in df.real_addition_terms]\n",
    "num_false_addition_terms = [len(literal_eval(ls)) for ls in df.false_addition_terms]\n",
    "num_real_deletion_terms = [len(literal_eval(ls)) for ls in df.real_deletion_terms]\n",
    "num_false_deletion_terms = [len(literal_eval(ls)) for ls in df.false_deletion_terms]\n",
    "num_replacement_terms = [len(literal_eval(ls)) for ls in df.replacement_terms]\n",
    "num_new_ingre_not_matched = [len(literal_eval(ls)) for ls in df.new_ingre_not_matched]\n",
    "num_full_match = [len(literal_eval(ls)) for ls in df.full_match]\n",
    "num_partial_match = [len(literal_eval(ls)) for ls in df.partial_match]\n",
    "num_synonym_match = [len(literal_eval(ls)) for ls in df.synonym_match]\n",
    "num_parent_match = [len(literal_eval(ls)) for ls in df.parent_match]\n",
    "num_child_match = [len(literal_eval(ls)) for ls in df.child_match]\n",
    "num_new_spice = [len(literal_eval(ls)) for ls in df.new_spice]\n",
    "stats_df_seq1 = pd.DataFrame({\n",
    "    'review_id': df.review_id,\n",
    "    'num_ingre_in_recipe':num_ingre_in_recipe,\n",
    "    'num_ingre_not_recipe':num_ingre_not_recipe,\n",
    "    'num_real_addition_terms':num_real_addition_terms,\n",
    "    'num_false_addition_terms':num_false_addition_terms,\n",
    "    'num_real_deletion_terms':num_real_deletion_terms,\n",
    "    'num_false_deletion_terms':num_false_deletion_terms,\n",
    "    'num_replacement_terms':num_replacement_terms,\n",
    "    'num_new_ingre_not_matched':num_new_ingre_not_matched,\n",
    "    'num_full_match':num_full_match,\n",
    "    'num_partial_match':num_partial_match,\n",
    "    'num_synonym_match':num_synonym_match,\n",
    "    'num_parent_match':num_parent_match,\n",
    "    'num_child_match':num_child_match,\n",
    "    'num_new_spice':num_new_spice\n",
    "})\n",
    "\n",
    "stats_df_seq1.to_csv('OUTPUT_OF_FINAL_CODE/stats_altering_seq1.csv')\n",
    "\n",
    "# result for seq2:\n",
    "df = result_seq2\n",
    "\n",
    "num_ingre_in_recipe = [len(literal_eval(ls)) for ls in df.ingre_in_recipe]\n",
    "num_ingre_not_recipe = [len(literal_eval(ls)) for ls in df.ingre_not_recipe]\n",
    "num_real_addition_terms = [len(literal_eval(ls)) for ls in df.real_addition_terms]\n",
    "num_false_addition_terms = [len(literal_eval(ls)) for ls in df.false_addition_terms]\n",
    "num_real_deletion_terms = [len(literal_eval(ls)) for ls in df.real_deletion_terms]\n",
    "num_false_deletion_terms = [len(literal_eval(ls)) for ls in df.false_deletion_terms]\n",
    "num_replacement_terms = [len(literal_eval(ls)) for ls in df.replacement_terms]\n",
    "num_new_ingre_not_matched = [len(literal_eval(ls)) for ls in df.new_ingre_not_matched]\n",
    "num_full_match = [len(literal_eval(ls)) for ls in df.full_match]\n",
    "num_partial_match = [len(literal_eval(ls)) for ls in df.partial_match]\n",
    "num_synonym_match = [len(literal_eval(ls)) for ls in df.synonym_match]\n",
    "num_parent_match = [len(literal_eval(ls)) for ls in df.parent_match]\n",
    "num_child_match = [len(literal_eval(ls)) for ls in df.child_match]\n",
    "num_new_spice = [len(literal_eval(ls)) for ls in df.new_spice]\n",
    "stats_df_seq2 = pd.DataFrame({\n",
    "    'review_id': df.review_id,\n",
    "    'num_ingre_in_recipe':num_ingre_in_recipe,\n",
    "    'num_ingre_not_recipe':num_ingre_not_recipe,\n",
    "    'num_real_addition_terms':num_real_addition_terms,\n",
    "    'num_false_addition_terms':num_false_addition_terms,\n",
    "    'num_real_deletion_terms':num_real_deletion_terms,\n",
    "    'num_false_deletion_terms':num_false_deletion_terms,\n",
    "    'num_replacement_terms':num_replacement_terms,\n",
    "    'num_new_ingre_not_matched':num_new_ingre_not_matched,\n",
    "    'num_full_match':num_full_match,\n",
    "    'num_partial_match':num_partial_match,\n",
    "    'num_synonym_match':num_synonym_match,\n",
    "    'num_parent_match':num_parent_match,\n",
    "    'num_child_match':num_child_match,\n",
    "    'num_new_spice':num_new_spice\n",
    "})\n",
    "\n",
    "stats_df_seq2.to_csv('OUTPUT_OF_FINAL_CODE/stats_altering_seq2.csv')\n",
    "\n",
    "\n",
    "# result for seq3:\n",
    "df = result_seq3\n",
    "\n",
    "num_ingre_in_recipe = [len(literal_eval(ls)) for ls in df.ingre_in_recipe]\n",
    "num_ingre_not_recipe = [len(literal_eval(ls)) for ls in df.ingre_not_recipe]\n",
    "num_real_addition_terms = [len(literal_eval(ls)) for ls in df.real_addition_terms]\n",
    "num_false_addition_terms = [len(literal_eval(ls)) for ls in df.false_addition_terms]\n",
    "num_real_deletion_terms = [len(literal_eval(ls)) for ls in df.real_deletion_terms]\n",
    "num_false_deletion_terms = [len(literal_eval(ls)) for ls in df.false_deletion_terms]\n",
    "num_replacement_terms = [len(literal_eval(ls)) for ls in df.replacement_terms]\n",
    "num_new_ingre_not_matched = [len(literal_eval(ls)) for ls in df.new_ingre_not_matched]\n",
    "num_full_match = [len(literal_eval(ls)) for ls in df.full_match]\n",
    "num_partial_match = [len(literal_eval(ls)) for ls in df.partial_match]\n",
    "num_synonym_match = [len(literal_eval(ls)) for ls in df.synonym_match]\n",
    "num_parent_match = [len(literal_eval(ls)) for ls in df.parent_match]\n",
    "num_child_match = [len(literal_eval(ls)) for ls in df.child_match]\n",
    "num_new_spice = [len(literal_eval(ls)) for ls in df.new_spice]\n",
    "stats_df_seq3 = pd.DataFrame({\n",
    "    'review_id': df.review_id,\n",
    "    'num_ingre_in_recipe':num_ingre_in_recipe,\n",
    "    'num_ingre_not_recipe':num_ingre_not_recipe,\n",
    "    'num_real_addition_terms':num_real_addition_terms,\n",
    "    'num_false_addition_terms':num_false_addition_terms,\n",
    "    'num_real_deletion_terms':num_real_deletion_terms,\n",
    "    'num_false_deletion_terms':num_false_deletion_terms,\n",
    "    'num_replacement_terms':num_replacement_terms,\n",
    "    'num_new_ingre_not_matched':num_new_ingre_not_matched,\n",
    "    'num_full_match':num_full_match,\n",
    "    'num_partial_match':num_partial_match,\n",
    "    'num_synonym_match':num_synonym_match,\n",
    "    'num_parent_match':num_parent_match,\n",
    "    'num_child_match':num_child_match,\n",
    "    'num_new_spice':num_new_spice\n",
    "})\n",
    "\n",
    "stats_df_seq3.to_csv('OUTPUT_OF_FINAL_CODE/stats_altering_seq3.csv')\n",
    "\n",
    "\n",
    "merge_stats_1_2 = pd.merge(stats_df_seq1, stats_df_seq2, how='left', on = 'review_id', \n",
    "                                 suffixes = ['_1', '_2'])\n",
    "merge_stats_1_2_3 = pd.merge(merge_stats_1_2, stats_df_seq3.add_suffix('_3'), how='left', left_on = 'review_id',\n",
    "                             right_on = 'review_id_3',suffixes = ['', '']).drop(['review_id_3'], axis=1)\n",
    "merge_stats_1_2_3.to_csv('OUTPUT_OF_FINAL_CODE/stats_altering_combined.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
